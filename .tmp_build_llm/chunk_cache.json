{
  "version": 2,
  "files": {
    "DESIGN.md": {
      "source_hash": "76496374e0a76a6adcc3d5f55eb816518c7ab794e350103f455225215e95335b",
      "classification": {
        "pipeline": "markdown",
        "language": "markdown",
        "mime": "text/markdown"
      },
      "segments": [
        {
          "id": "DESIGN.md:heading_section:1:1a9c53297d",
          "kind": "heading_section",
          "text": "# cpm-llm-builder Design Notes",
          "start": 1,
          "end": 2,
          "symbol": "cpm-llm-builder Design Notes",
          "metadata": {}
        },
        {
          "id": "DESIGN.md:heading_section:3:7a1e8e779e",
          "kind": "heading_section",
          "text": "## Current Pipeline (v2 foundation)\n\n1. Ingest file text.\n2. Classify file type/language/mime.\n3. Deterministic pre-chunking (code/doc/json/text).\n4. LLM enrichment via OpenAI-like envelope over HTTP.\n5. Postprocess split/merge by token constraints.\n6. Validate chunks and collect warnings.\n7. Cache v2:\n   - file-level segmentation cache\n   - segment-level enrichment cache\n8. Produce CPM packet artifacts with embedding incremental reuse.",
          "start": 3,
          "end": 15,
          "symbol": "Current Pipeline (v2 foundation)",
          "metadata": {}
        },
        {
          "id": "DESIGN.md:heading_section:16:a1898e947b",
          "kind": "heading_section",
          "text": "## Compatibility\n\n- Supports legacy chunk responses:\n  - `[\"chunk1\", \"chunk2\"]`\n  - `{\"chunks\": [...]}`\n- Supports OpenAI-like envelopes with `output_json`.",
          "start": 16,
          "end": 22,
          "symbol": "Compatibility",
          "metadata": {}
        },
        {
          "id": "DESIGN.md:heading_section:23:8430825f5e",
          "kind": "heading_section",
          "text": "## Future\n\n- Java AST parser integration (tree-sitter) can replace regex pre-chunker.\n- Circuit breaker and advanced transport options can be layered into `llm_client.py`.",
          "start": 23,
          "end": 26,
          "symbol": "Future",
          "metadata": {}
        }
      ]
    },
    "README.md": {
      "source_hash": "1d8614975d324c9b315dab2b62ad36f217372fa05547392977b52deb47542c45",
      "classification": {
        "pipeline": "markdown",
        "language": "markdown",
        "mime": "text/markdown"
      },
      "segments": [
        {
          "id": "README.md:heading_section:1:58a8c5e3a1",
          "kind": "heading_section",
          "text": "# cpm-llm-builder\n\n`cpm-llm-builder` is a CPM plugin that builds packets with a deterministic pre-chunk pipeline and LLM enrichment over HTTP.",
          "start": 1,
          "end": 4,
          "symbol": "cpm-llm-builder",
          "metadata": {}
        },
        {
          "id": "README.md:heading_section:5:a207346546",
          "kind": "heading_section",
          "text": "## Pipeline\n\n1. Ingest files.\n2. Classify file type/language/mime.\n3. Deterministic pre-chunk:\n   - `java`\n   - `code_generic`\n   - `markdown/html`\n   - `json/yaml`\n   - `text`\n4. LLM enrichment (metadata, title, summary, tags, relations).\n5. Postprocess split/merge by token constraints.\n6. Quality validation.\n7. Cache v2 (file + segment enrichment).",
          "start": 5,
          "end": 19,
          "symbol": "Pipeline",
          "metadata": {}
        },
        {
          "id": "README.md:heading_section:20:0ea2c21c7d",
          "kind": "heading_section",
          "text": "## Endpoint Contract\n\nThe plugin sends an OpenAI-like envelope:\n\n```json\n{\n  \"model\": \"chunker-xxx\",\n  \"input\": [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": \"instructions\"},\n        {\n          \"type\": \"input_json\",\n          \"json\": {\n            \"task\": \"chunk.enrich\",\n            \"source\": {\"path\": \"...\", \"language\": \"java\", \"mime\": \"text/x-java\", \"hash\": \"...\"},\n            \"segments\": [{\"id\": \"...\", \"kind\": \"method\", \"text\": \"...\", \"start\": 10, \"end\": 20}],\n            \"constraints\": {\"max_chunk_tokens\": 800, \"min_chunk_tokens\": 120, \"max_segments_per_request\": 8}\n          }\n        }\n      ]\n    }\n  ],\n  \"metadata\": {\"cpm_plugin\": \"cpm-llm-builder\", \"prompt_version\": \"chunk_enrich_v1\"}\n}\n```\n\nAccepted response formats:\n- OpenAI-like:\n```json\n{\"output\":[{\"type\":\"output_json\",\"json\":{\"chunks\":[{\"id\":\"...\",\"text\":\"...\"}]}}]}\n```\n- Legacy:\n```json\n{\"chunks\":[{\"id\":\"...\",\"text\":\"...\"}]}\n```\nor\n```json\n[\"chunk text 1\", \"chunk text 2\"]\n```",
          "start": 20,
          "end": 61,
          "symbol": "Endpoint Contract",
          "metadata": {}
        },
        {
          "id": "README.md:heading_section:62:a8844e60da",
          "kind": "heading_section",
          "text": "## Config\n\n`config.yml`:\n\n```yaml\nllm:\n  endpoint: \"http://127.0.0.1:9000/chunk\"\n  model: \"chunker-xxx\"\n  prompt_version: \"chunk_enrich_v1\"\n  max_retries: 2\nrequest_timeout: 30.0\nconstraints:\n  max_chunk_tokens: 800\n  min_chunk_tokens: 120\n  max_segments_per_request: 8\n```",
          "start": 62,
          "end": 78,
          "symbol": "Config",
          "metadata": {}
        },
        {
          "id": "README.md:heading_section:79:a19114bd3a",
          "kind": "heading_section",
          "text": "## Run\n\n```bash\ncpm llm:cpm-llm-builder ./docs --destination ./out/packet\n```",
          "start": 79,
          "end": 83,
          "symbol": "Run",
          "metadata": {}
        }
      ]
    },
    "__init__.py": {
      "source_hash": "76eea5ae9de48343c567e52beebbc1f7bce097df69d7ce1bd88320d69d43b17a",
      "classification": {
        "pipeline": "code_generic",
        "language": "python",
        "mime": "text/x-python"
      },
      "segments": [
        {
          "id": "__init__.py:code_block:1:76eea5ae9d",
          "kind": "code_block",
          "text": "\"\"\"LLM builder plugin package.\"\"\"",
          "start": 1,
          "end": 2,
          "symbol": null,
          "metadata": {}
        }
      ]
    },
    "config.yml": {
      "source_hash": "e8b477a7c1762a67cafe38e50cbf6ad65972e6ad35cba444febdb46a5d5e1600",
      "classification": {
        "pipeline": "yaml",
        "language": "yaml",
        "mime": "application/yaml"
      },
      "segments": [
        {
          "id": "config.yml:top_level_key:1:31fecb481a",
          "kind": "top_level_key",
          "text": "llm:\nendpoint: http://localhost:11434/v1/chat/completions\nmax_retries: 2\nmodel: qwen2.5-coder:7b-instruct\nprompt_version: chunk_enrich_v1",
          "start": 1,
          "end": 12,
          "symbol": "llm",
          "metadata": {}
        },
        {
          "id": "config.yml:top_level_key:1:a5a2cb6f08",
          "kind": "top_level_key",
          "text": "request_timeout:\n30.0\n...",
          "start": 1,
          "end": 12,
          "symbol": "request_timeout",
          "metadata": {}
        },
        {
          "id": "config.yml:top_level_key:1:57d4efb39a",
          "kind": "top_level_key",
          "text": "constraints:\nmax_chunk_tokens: 800\nmax_segments_per_request: 8\nmin_chunk_tokens: 120",
          "start": 1,
          "end": 12,
          "symbol": "constraints",
          "metadata": {}
        }
      ]
    },
    "cpm_llm_builder_plugin/__init__.py": {
      "source_hash": "7d2fad9b79354a0384920a9c65adce95e5595767ebd404c87bed0d2a66a8bfea",
      "classification": {
        "pipeline": "code_generic",
        "language": "python",
        "mime": "text/x-python"
      },
      "segments": [
        {
          "id": "cpm_llm_builder_plugin/__init__.py:code_block:1:7d2fad9b79",
          "kind": "code_block",
          "text": "\"\"\"CPM plugin that chunks files with an external LLM endpoint before embedding.\"\"\"",
          "start": 1,
          "end": 2,
          "symbol": null,
          "metadata": {}
        }
      ]
    },
    "cpm_llm_builder_plugin/cache.py": {
      "source_hash": "18f90956ba6379a81eaec664c6ec1e997a6701f6a51fe63bc7e22b223e208ee4",
      "classification": {
        "pipeline": "code_generic",
        "language": "python",
        "mime": "text/x-python"
      },
      "segments": [
        {
          "id": "cpm_llm_builder_plugin/cache.py:code_symbol:17:87726a0eb5",
          "kind": "code_symbol",
          "text": "class FileCacheEntry:\n    source_hash: str\n    classification: dict[str, Any] = field(default_factory=dict)\n    segments: list[Segment] = field(default_factory=list)\n\n\n@dataclass",
          "start": 17,
          "end": 23,
          "symbol": "FileCacheEntry",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/cache.py:code_symbol:24:ed746e53b3",
          "kind": "code_symbol",
          "text": "class CacheV2:\n    files: dict[str, FileCacheEntry] = field(default_factory=dict)\n    segment_enrichment: dict[str, Chunk] = field(default_factory=dict)",
          "start": 24,
          "end": 28,
          "symbol": "CacheV2",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/cache.py:code_symbol:29:ef5b2856da",
          "kind": "code_symbol",
          "text": "def load_cache(path: Path) -> CacheV2:\n    if not path.exists():\n        return CacheV2()\n    try:\n        payload = json.loads(path.read_text(encoding=\"utf-8\"))\n    except Exception:\n        return CacheV2()\n\n    if isinstance(payload, Mapping) and payload.get(\"version\") == CACHE_VERSION:\n        return _load_v2(payload)\n\n    # v1 migration: {\"files\": {\"path\": {\"source_hash\": \"...\", \"chunks\":[...]}}}\n    migrated = CacheV2()\n    files_raw = payload.get(\"files\") if isinstance(payload, Mapping) else None\n    if isinstance(files_raw, Mapping):\n        for rel, entry in files_raw.items():\n            if not isinstance(rel, str) or not isinstance(entry, Mapping):\n                continue\n            source_hash = entry.get(\"source_hash\")\n            chunks = entry.get(\"chunks\")\n            if not isinstance(source_hash, str):\n                continue\n            segments: list[Segment] = []\n            if isinstance(chunks, list):\n                for idx, item in enumerate(chunks):\n                    if not isinstance(item, str):\n                        continue\n                    text = item.strip()\n                    if not text:\n                        continue\n                    segments.append(\n                        Segment(\n                            id=f\"{rel}:legacy:{idx}\",\n                            kind=\"legacy_chunk\",\n                            text=text,\n                            start_line=1,\n                            end_line=1,\n                            metadata={},\n                        )\n                    )\n            migrated.files[rel] = FileCacheEntry(\n                source_hash=source_hash,\n                classification={\"pipeline\": \"legacy\"},\n                segments=segments,\n            )\n    return migrated",
          "start": 29,
          "end": 76,
          "symbol": "load_cache",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/cache.py:code_symbol:77:d915515a66",
          "kind": "code_symbol",
          "text": "def _load_v2(payload: Mapping[str, Any]) -> CacheV2:\n    result = CacheV2()\n    files_raw = payload.get(\"files\")\n    if isinstance(files_raw, Mapping):\n        for rel, value in files_raw.items():\n            if not isinstance(rel, str) or not isinstance(value, Mapping):\n                continue\n            source_hash = value.get(\"source_hash\")\n            if not isinstance(source_hash, str):\n                continue\n            cls = dict(value.get(\"classification\") or {})\n            seg_payload = value.get(\"segments\") or []\n            segments = []\n            if isinstance(seg_payload, list):\n                for item in seg_payload:\n                    if isinstance(item, Mapping):\n                        try:\n                            segments.append(Segment.from_dict(item))\n                        except Exception:\n                            continue\n            result.files[rel] = FileCacheEntry(\n                source_hash=source_hash,\n                classification=cls,\n                segments=segments,\n            )\n\n    seg_enrichment = payload.get(\"segment_enrichment\")\n    if isinstance(seg_enrichment, Mapping):\n        for key, value in seg_enrichment.items():\n            if not isinstance(key, str) or not isinstance(value, Mapping):\n                continue\n            try:\n                result.segment_enrichment[key] = Chunk.from_dict(value)\n            except Exception:\n                continue\n    return result",
          "start": 77,
          "end": 114,
          "symbol": "_load_v2",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/cache.py:code_symbol:115:ac27c9054f",
          "kind": "code_symbol",
          "text": "def save_cache(path: Path, cache: CacheV2) -> None:\n    payload = {\n        \"version\": CACHE_VERSION,\n        \"files\": {\n            rel: {\n                \"source_hash\": entry.source_hash,\n                \"classification\": dict(entry.classification),\n                \"segments\": [segment.to_dict() for segment in entry.segments],\n            }\n            for rel, entry in sorted(cache.files.items())\n        },\n        \"segment_enrichment\": {\n            key: value.to_dict()\n            for key, value in sorted(cache.segment_enrichment.items())\n        },\n    }\n    path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding=\"utf-8\")",
          "start": 115,
          "end": 132,
          "symbol": "save_cache",
          "metadata": {}
        }
      ]
    },
    "cpm_llm_builder_plugin/classifiers.py": {
      "source_hash": "d36733895a438d3b15b5f4afe5d5a24e9ccb936558cac338ecdcaa6cb45428af",
      "classification": {
        "pipeline": "code_generic",
        "language": "python",
        "mime": "text/x-python"
      },
      "segments": [
        {
          "id": "cpm_llm_builder_plugin/classifiers.py:code_symbol:10:25cda329cb",
          "kind": "code_symbol",
          "text": "class FileClassification:\n    pipeline: str\n    language: str\n    mime: str\n    is_supported_text: bool = True\n\n\nPIPELINES_BY_EXT: dict[str, FileClassification] = {\n    \".java\": FileClassification(\"java\", \"java\", \"text/x-java\"),\n    \".md\": FileClassification(\"markdown\", \"markdown\", \"text/markdown\"),\n    \".markdown\": FileClassification(\"markdown\", \"markdown\", \"text/markdown\"),\n    \".html\": FileClassification(\"html\", \"html\", \"text/html\"),\n    \".htm\": FileClassification(\"html\", \"html\", \"text/html\"),\n    \".json\": FileClassification(\"json\", \"json\", \"application/json\"),\n    \".yml\": FileClassification(\"yaml\", \"yaml\", \"application/yaml\"),\n    \".yaml\": FileClassification(\"yaml\", \"yaml\", \"application/yaml\"),\n    \".txt\": FileClassification(\"text\", \"text\", \"text/plain\"),\n    \".rst\": FileClassification(\"text\", \"rst\", \"text/plain\"),\n    \".py\": FileClassification(\"code_generic\", \"python\", \"text/x-python\"),\n    \".js\": FileClassification(\"code_generic\", \"javascript\", \"text/javascript\"),\n    \".ts\": FileClassification(\"code_generic\", \"typescript\", \"text/typescript\"),\n    \".tsx\": FileClassification(\"code_generic\", \"typescript\", \"text/typescript\"),\n    \".go\": FileClassification(\"code_generic\", \"go\", \"text/x-go\"),\n    \".rs\": FileClassification(\"code_generic\", \"rust\", \"text/x-rust\"),\n    \".c\": FileClassification(\"code_generic\", \"c\", \"text/x-c\"),\n    \".cpp\": FileClassification(\"code_generic\", \"cpp\", \"text/x-c++\"),\n    \".h\": FileClassification(\"code_generic\", \"c\", \"text/x-c\"),\n    \".cs\": FileClassification(\"code_generic\", \"csharp\", \"text/x-csharp\"),\n    \".kt\": FileClassification(\"code_generic\", \"kotlin\", \"text/x-kotlin\"),\n}",
          "start": 10,
          "end": 41,
          "symbol": "FileClassification",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/classifiers.py:code_symbol:42:ea72afdcb9",
          "kind": "code_symbol",
          "text": "def classify_file(path: Path, content: str) -> FileClassification:\n    ext = path.suffix.lower()\n    known = PIPELINES_BY_EXT.get(ext)\n    if known is not None:\n        return known\n\n    head = content.lstrip()[:128]\n    if head.startswith(\"#!\"):\n        if \"python\" in head:\n            return FileClassification(\"code_generic\", \"python\", \"text/x-python\")\n        if \"bash\" in head or \"sh\" in head:\n            return FileClassification(\"text\", \"shell\", \"text/x-shellscript\")\n\n    if \"\\x00\" in content:\n        return FileClassification(\n            pipeline=\"binary_unsupported\",\n            language=\"binary\",\n            mime=\"application/octet-stream\",\n            is_supported_text=False,\n        )\n\n    return FileClassification(\"text\", \"text\", \"text/plain\")",
          "start": 42,
          "end": 64,
          "symbol": "classify_file",
          "metadata": {}
        }
      ]
    },
    "cpm_llm_builder_plugin/entrypoint.py": {
      "source_hash": "6f6bfef697662d5fd7efaf46d3dffac4a5d80d5233fd0de5b0f63d3808ad793f",
      "classification": {
        "pipeline": "code_generic",
        "language": "python",
        "mime": "text/x-python"
      },
      "segments": [
        {
          "id": "cpm_llm_builder_plugin/entrypoint.py:code_symbol:8:590d188334",
          "kind": "code_symbol",
          "text": "class LLMBuilderEntrypoint:\n    \"\"\"Initialize plugin state and register features.\"\"\"",
          "start": 8,
          "end": 10,
          "symbol": "LLMBuilderEntrypoint",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/entrypoint.py:code_symbol:11:4a3baf7526",
          "kind": "code_symbol",
          "text": "def init(self, ctx) -> None:\n        self.context = ctx\n        features.set_plugin_root(ctx.plugin_root)\n        _ = features.CPMLLMBuilder",
          "start": 11,
          "end": 15,
          "symbol": "init",
          "metadata": {}
        }
      ]
    },
    "cpm_llm_builder_plugin/features.py": {
      "source_hash": "b666a60d5928da977a5ac70efd6dab230a8c3e503d012e9286976ce5bde989b3",
      "classification": {
        "pipeline": "code_generic",
        "language": "python",
        "mime": "text/x-python"
      },
      "segments": [
        {
          "id": "cpm_llm_builder_plugin/features.py:code_symbol:46:6ab4ccacc6",
          "kind": "code_symbol",
          "text": "def set_plugin_root(path: Path) -> None:\n    global _PLUGIN_ROOT\n    _PLUGIN_ROOT = path",
          "start": 46,
          "end": 50,
          "symbol": "set_plugin_root",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/features.py:code_symbol:51:5706db2674",
          "kind": "code_symbol",
          "text": "def _sha256_text(value: str) -> str:\n    return hashlib.sha256(value.encode(\"utf-8\")).hexdigest()",
          "start": 51,
          "end": 54,
          "symbol": "_sha256_text",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/features.py:code_symbol:55:fcdb002f55",
          "kind": "code_symbol",
          "text": "def _resolve_config_path(config_arg: str | None) -> Path:\n    if config_arg:\n        return Path(config_arg).expanduser().resolve()\n    if _PLUGIN_ROOT is not None:\n        return (_PLUGIN_ROOT / DEFAULT_CONFIG_NAME).resolve()\n    return Path(DEFAULT_CONFIG_NAME).resolve()\n\n\n@dataclass(frozen=True)",
          "start": 55,
          "end": 63,
          "symbol": "_resolve_config_path",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/features.py:code_symbol:64:1efecd58ec",
          "kind": "code_symbol",
          "text": "class LLMBuilderPluginConfig:\n    llm_endpoint: str\n    request_timeout: float = 30.0\n    llm_model: str = \"chunker-xxx\"\n    prompt_version: str = \"chunk_enrich_v1\"\n    max_retries: int = 2\n    max_chunk_tokens: int = 800\n    min_chunk_tokens: int = 120\n    max_segments_per_request: int = 8\n\n    @classmethod",
          "start": 64,
          "end": 74,
          "symbol": "LLMBuilderPluginConfig",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/features.py:code_symbol:75:1863a044ea",
          "kind": "code_symbol",
          "text": "def from_path(cls, path: Path) -> \"LLMBuilderPluginConfig\":\n        payload = yaml.safe_load(path.read_text(encoding=\"utf-8\")) or {}\n        if not isinstance(payload, dict):\n            raise ValueError(\"config.yml must contain a mapping\")\n\n        legacy_endpoint = str(payload.get(\"llm_endpoint\") or \"\").strip()\n        llm_cfg = payload.get(\"llm\") if isinstance(payload.get(\"llm\"), dict) else {}\n        endpoint = str(llm_cfg.get(\"endpoint\") or legacy_endpoint).strip()\n        if not endpoint:\n            raise ValueError(\"config.yml must define llm.endpoint or llm_endpoint\")\n\n        constraints = payload.get(\"constraints\") if isinstance(payload.get(\"constraints\"), dict) else {}\n\n        return cls(\n            llm_endpoint=endpoint,\n            request_timeout=float(payload.get(\"request_timeout\", 30.0)),\n            llm_model=str(llm_cfg.get(\"model\") or \"chunker-xxx\"),\n            prompt_version=str(llm_cfg.get(\"prompt_version\") or \"chunk_enrich_v1\"),\n            max_retries=int(llm_cfg.get(\"max_retries\", 2)),\n            max_chunk_tokens=int(constraints.get(\"max_chunk_tokens\", 800)),\n            min_chunk_tokens=int(constraints.get(\"min_chunk_tokens\", 120)),\n            max_segments_per_request=int(constraints.get(\"max_segments_per_request\", 8)),\n        )\n\n\n@dataclass(frozen=True)",
          "start": 75,
          "end": 100,
          "symbol": "from_path",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/features.py:code_symbol:101:821fa67b67",
          "kind": "code_symbol",
          "text": "class LLMBuilderRuntimeConfig:\n    llm_endpoint: str\n    request_timeout: float\n    llm_model: str\n    prompt_version: str\n    max_retries: int\n    constraints: ChunkConstraints\n    model_name: str\n    max_seq_length: int\n    version: str\n    archive: bool\n    archive_format: str\n    embed_url: str\n    embeddings_mode: str\n    timeout: float | None\n\n\n@cpmbuilder(name=\"cpm-llm-builder\", group=\"llm\")",
          "start": 101,
          "end": 118,
          "symbol": "LLMBuilderRuntimeConfig",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/features.py:code_symbol:119:002670001a",
          "kind": "code_symbol",
          "text": "class CPMLLMBuilder(CPMAbstractBuilder):\n    @classmethod",
          "start": 119,
          "end": 120,
          "symbol": "CPMLLMBuilder",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/features.py:code_symbol:121:bc58ffc84a",
          "kind": "code_symbol",
          "text": "def configure(cls, parser: argparse.ArgumentParser) -> None:\n        parser.add_argument(\"source\", help=\"Source directory to build\")\n        parser.add_argument(\"--destination\", required=True, help=\"Destination packet directory\")\n        parser.add_argument(\"--packet-version\", default=\"0.0.0\", help=\"Packet version for manifest/cpm.yml\")\n        parser.add_argument(\"--config\", help=\"Plugin config.yml path (default: plugin config.yml)\")\n        parser.add_argument(\"--llm-endpoint\", help=\"Override LLM endpoint from config\")\n        parser.add_argument(\"--request-timeout\", type=float, help=\"Timeout in seconds for LLM calls\")\n        parser.add_argument(\"--llm-model\", help=\"Override LLM model\")\n        parser.add_argument(\"--prompt-version\", help=\"Override prompt version\")\n        parser.add_argument(\"--max-retries\", type=int, help=\"LLM retries\")\n        parser.add_argument(\"--max-chunk-tokens\", type=int, help=\"Chunk hard max size\")\n        parser.add_argument(\"--min-chunk-tokens\", type=int, help=\"Chunk soft min size\")\n        parser.add_argument(\"--max-segments-per-request\", type=int, help=\"LLM batch size\")\n        parser.add_argument(\"--model-name\", default=DEFAULT_MODEL, help=\"Embedding model name\")\n        parser.add_argument(\"--max-seq-length\", type=int, default=1024, help=\"Embedding max sequence length\")\n        parser.add_argument(\"--embed-url\", default=DEFAULT_EMBED_URL, help=\"Embedding endpoint URL\")\n        parser.add_argument(\"--embeddings-mode\", choices=[\"http\", \"legacy\"], default=\"http\")\n        parser.add_argument(\"--timeout\", type=float, default=None, help=\"Embedding request timeout in seconds\")\n        parser.add_argument(\"--archive\", dest=\"archive\", action=\"store_true\", help=\"Create archive output\")\n        parser.add_argument(\"--no-archive\", dest=\"archive\", action=\"store_false\", help=\"Skip archive output\")\n        parser.set_defaults(archive=True)\n        parser.add_argument(\"--archive-format\", choices=[\"tar.gz\", \"zip\"], default=\"tar.gz\")",
          "start": 121,
          "end": 143,
          "symbol": "configure",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/features.py:code_symbol:144:33d424f6c2",
          "kind": "code_symbol",
          "text": "def __init__(self, config: LLMBuilderRuntimeConfig | None = None, *, embedder: Any | None = None) -> None:\n        self.config = config\n        self.embedder = embedder",
          "start": 144,
          "end": 147,
          "symbol": "__init__",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/features.py:code_symbol:148:ffeafa9b5c",
          "kind": "code_symbol",
          "text": "def run(self, argv: Sequence[str]) -> int:\n        args = argv\n        config_path = _resolve_config_path(getattr(args, \"config\", None))\n        if not config_path.exists():\n            print(f\"[error] config file not found: {config_path}\")\n            return 1\n        try:\n            base = LLMBuilderPluginConfig.from_path(config_path)\n        except Exception as exc:\n            print(f\"[error] invalid config.yml: {exc}\")\n            return 1\n\n        constraints = ChunkConstraints(\n            max_chunk_tokens=int(getattr(args, \"max_chunk_tokens\", None) or base.max_chunk_tokens),\n            min_chunk_tokens=int(getattr(args, \"min_chunk_tokens\", None) or base.min_chunk_tokens),\n            max_segments_per_request=int(\n                getattr(args, \"max_segments_per_request\", None) or base.max_segments_per_request\n            ),\n        )\n\n        runtime = LLMBuilderRuntimeConfig(\n            llm_endpoint=str(getattr(args, \"llm_endpoint\", None) or base.llm_endpoint),\n            request_timeout=float(getattr(args, \"request_timeout\", None) or base.request_timeout),\n            llm_model=str(getattr(args, \"llm_model\", None) or base.llm_model),\n            prompt_version=str(getattr(args, \"prompt_version\", None) or base.prompt_version),\n            max_retries=int(getattr(args, \"max_retries\", None) or base.max_retries),\n            constraints=constraints,\n            model_name=str(getattr(args, \"model_name\", None) or DEFAULT_MODEL),\n            max_seq_length=int(getattr(args, \"max_seq_length\", None) or 1024),\n            version=str(getattr(args, \"packet_version\", None) or \"0.0.0\"),\n            archive=bool(getattr(args, \"archive\", True)),\n            archive_format=str(getattr(args, \"archive_format\", None) or \"tar.gz\"),\n            embed_url=str(getattr(args, \"embed_url\", None) or DEFAULT_EMBED_URL),\n            embeddings_mode=str(getattr(args, \"embeddings_mode\", None) or \"http\"),\n            timeout=getattr(args, \"timeout\", None),\n        )\n        self.config = runtime\n        self.embedder = self.embedder or EmbeddingClient(\n            base_url=runtime.embed_url,\n            mode=runtime.embeddings_mode,\n            timeout_s=runtime.timeout,\n        )\n        manifest = self.build(str(getattr(args, \"source\")), destination=str(getattr(args, \"destination\")))\n        return 0 if manifest is not None else 1",
          "start": 148,
          "end": 192,
          "symbol": "run",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/features.py:code_symbol:193:7a3392cf78",
          "kind": "code_symbol",
          "text": "def _fallback_chunk(self, *, source: SourceDocument, segment_text: str, segment_id: str, start: int, end: int) -> Chunk:\n        return Chunk(\n            id=segment_id,\n            text=segment_text,\n            title=\"\",\n            summary=\"\",\n            tags=(),\n            anchors={\"path\": source.path, \"start_line\": start, \"end_line\": end},\n            relations={},\n            metadata={\"fallback\": True},\n        )",
          "start": 193,
          "end": 204,
          "symbol": "_fallback_chunk",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/features.py:code_symbol:205:8aa6297f8f",
          "kind": "code_symbol",
          "text": "def build(self, source: str, *, destination: str | None = None) -> PacketManifest | None:\n        if self.config is None:\n            raise ValueError(\"runtime config is not initialized\")\n        if self.embedder is None:\n            self.embedder = EmbeddingClient(\n                base_url=self.config.embed_url,\n                mode=self.config.embeddings_mode,\n                timeout_s=self.config.timeout,\n            )\n\n        source_path = Path(source).resolve()\n        if not source_path.exists():\n            print(f\"[error] source '{source_path}' does not exist\")\n            return None\n        if destination is None:\n            raise ValueError(\"destination path must be provided\")\n        out_root = Path(destination).resolve()\n        out_root.mkdir(parents=True, exist_ok=True)\n        (out_root / \"faiss\").mkdir(parents=True, exist_ok=True)\n        print(f\"[build] input_dir  = {source_path}\")\n        print(f\"[build] output_dir = {out_root}\")\n\n        llm_client = LLMClient(\n            LLMClientConfig(\n                endpoint=self.config.llm_endpoint,\n                model=self.config.llm_model,\n                request_timeout=self.config.request_timeout,\n                prompt_version=self.config.prompt_version,\n                max_retries=self.config.max_retries,\n            )\n        )\n\n        chunk_cache_path = out_root / CHUNK_CACHE_NAME\n        cache = load_cache(chunk_cache_path)\n        next_cache = CacheV2()\n        next_cache.segment_enrichment.update(cache.segment_enrichment)\n\n        chunks: list[DocChunk] = []\n        ext_counts: dict[str, int] = {}\n        files_indexed = 0\n        llm_calls = 0\n        file_cache_hits = 0\n        segment_cache_hits = 0\n\n        rel_root = source_path.resolve()\n        for file_path in sorted(source_path.rglob(\"*\")):\n            if not file_path.is_file():\n                continue\n            if file_path.suffix.lower() not in SUPPORTED_EXTS:\n                continue\n\n            text = _read_text_file(file_path)\n            if not text.strip():\n                continue\n            files_indexed += 1\n\n            rel = str(file_path.resolve().relative_to(rel_root)).replace(\"\\\\\", \"/\")\n            ext = file_path.suffix.lower()\n            ext_counts[ext] = ext_counts.get(ext, 0) + 1\n\n            classification = classify_file(file_path, text)\n            if not classification.is_supported_text:\n                continue\n\n            source_hash = _sha256_text(text)\n            cached_file = cache.files.get(rel)\n            if cached_file and cached_file.source_hash == source_hash:\n                segments = cached_file.segments\n                file_cache_hits += 1\n            else:\n                segments = prechunk(rel, text, classification)\n\n            next_cache.files[rel] = FileCacheEntry(\n                source_hash=source_hash,\n                classification={\n                    \"pipeline\": classification.pipeline,\n                    \"language\": classification.language,\n                    \"mime\": classification.mime,\n                },\n                segments=list(segments),\n            )\n            if not segments:\n                continue\n\n            source_doc = SourceDocument(\n                path=rel,\n                language=classification.language,\n                mime=classification.mime,\n                source_hash=source_hash,\n            )\n\n            resolved_chunks: list[Chunk] = []\n            missing_segments = []\n            missing_keys = []\n            for segment in segments:\n                key = segment_cache_key(\n                    segment=segment,\n                    model=self.config.llm_model,\n                    prompt_version=self.config.prompt_version,\n                    constraints=self.config.constraints,\n                )\n                cached = cache.segment_enrichment.get(key)\n                if cached is not None:\n                    resolved_chunks.append(cached)\n                    segment_cache_hits += 1\n                else:\n                    missing_segments.append(segment)\n                    missing_keys.append(key)\n\n            max_batch = max(1, self.config.constraints.max_segments_per_request)\n            for start in range(0, len(missing_segments), max_batch):\n                segment_batch = missing_segments[start : start + max_batch]\n                key_batch = missing_keys[start : start + max_batch]\n                try:\n                    enriched = llm_client.enrich(\n                        source=source_doc,\n                        segments=segment_batch,\n                        constraints=self.config.constraints,\n                    )\n                    llm_calls += 1\n                except Exception as exc:\n                    print(f\"[warn] llm enrichment failed for {rel}: {exc}; fallback enabled\")\n                    enriched = [\n                        self._fallback_chunk(\n                            source=source_doc,\n                            segment_text=segment.text,\n                            segment_id=segment.id,\n                            start=segment.start_line,\n                            end=segment.end_line,\n                        )\n                        for segment in segment_batch\n                    ]\n                for key, enriched_chunk in zip(key_batch, enriched):\n                    next_cache.segment_enrichment[key] = enriched_chunk\n                    resolved_chunks.append(enriched_chunk)\n\n            post = apply_chunk_constraints(resolved_chunks, self.config.constraints)\n            validation = validate_chunks(post)\n            for warning in validation.warnings:\n                print(f\"[warn] {rel}: {warning}\")\n\n            for chunk in validation.chunks:\n                meta = dict(chunk.metadata)\n                meta.update(\n                    {\n                        \"path\": rel,\n                        \"ext\": ext,\n                        \"title\": chunk.title,\n                        \"summary\": chunk.summary,\n                        \"tags\": list(chunk.tags),\n                        \"anchors\": dict(chunk.anchors),\n                        \"relations\": dict(chunk.relations),\n                    }\n                )\n                chunks.append(DocChunk(id=chunk.id, text=chunk.text, metadata=meta))\n\n        save_cache(chunk_cache_path, next_cache)\n        print(f\"[scan] files_indexed={files_indexed}\")\n        print(f\"[scan] chunks_total={len(chunks)}\")\n        print(f\"[chunk] llm_calls={llm_calls} file_cache_hits={file_cache_hits} segment_cache_hits={segment_cache_hits}\")\n        if not chunks:\n            print(\"[error] No chunks found.\")\n            return None\n\n        cache_pack = _load_existing_cache(\n            out_root,\n            model_name=self.config.model_name,\n            max_seq_length=self.config.max_seq_length,\n        )\n        cache_vecs: dict[str, np.ndarray] = {}\n        cache_dim: Optional[int] = None\n        if cache_pack:\n            cache_vecs, cache_dim = cache_pack\n            print(f\"[cache] enabled: cached_vectors={len(cache_vecs)} dim={cache_dim}\")\n        else:\n            print(\"[cache] disabled (no compatible previous build found)\")\n\n        new_hashes = [_chunk_hash(chunk.text) for chunk in chunks]\n        prev_set = set(cache_vecs.keys())\n        new_set = set(new_hashes)\n        removed = len(prev_set - new_set) if cache_vecs else 0\n        reused = sum(1 for hsh in new_hashes if hsh in cache_vecs)\n\n        to_embed_idx: list[int] = []\n        to_embed_texts: list[str] = []\n        for idx, hsh in enumerate(new_hashes):\n            if hsh not in cache_vecs:\n                to_embed_idx.append(idx)\n                to_embed_texts.append(chunks[idx].text)\n        print(f\"[cache] new_chunks={len(chunks)} reused={reused} to_embed={len(to_embed_idx)} removed={removed}\")\n\n        if not self.embedder.health():\n            print(\n                f\"[error] embedding server not reachable at {self.config.embed_url} (mode={self.config.embeddings_mode})\"\n            )\n            return None\n\n        vec_missing: Optional[np.ndarray] = None\n        dim: Optional[int] = cache_dim\n        if to_embed_texts:\n            vec_missing = self.embedder.embed_texts(\n                to_embed_texts,\n                model_name=self.config.model_name,\n                max_seq_length=self.config.max_seq_length,\n                normalize=True,\n                dtype=\"float32\",\n                show_progress=True,\n            )\n            dim = int(vec_missing.shape[1])\n        elif dim is None and chunks:\n            vec_missing = self.embedder.embed_texts(\n                [chunks[0].text],\n                model_name=self.config.model_name,\n                max_seq_length=self.config.max_seq_length,\n                normalize=True,\n                dtype=\"float32\",\n                show_progress=False,\n            )\n            dim = int(vec_missing.shape[1])\n            to_embed_idx = [0]\n        assert dim is not None\n\n        if cache_dim is not None and cache_dim != dim:\n            cache_vecs = {}\n            reused = 0\n            to_embed_idx = list(range(len(chunks)))\n            to_embed_texts = [chunk.text for chunk in chunks]\n            vec_missing = self.embedder.embed_texts(\n                to_embed_texts,\n                model_name=self.config.model_name,\n                max_seq_length=self.config.max_seq_length,\n                normalize=True,\n                dtype=\"float32\",\n                show_progress=True,\n            )\n            dim = int(vec_missing.shape[1])\n\n        final_vecs = np.empty((len(chunks), dim), dtype=np.float32)\n        if cache_vecs:\n            for idx, hsh in enumerate(new_hashes):\n                vector = cache_vecs.get(hsh)\n                if vector is not None:\n                    final_vecs[idx] = vector\n        if to_embed_idx:\n            assert vec_missing is not None\n            for miss_idx, chunk_idx in enumerate(to_embed_idx):\n                final_vecs[chunk_idx] = vec_missing[miss_idx]\n\n        docs_path = out_root / \"docs.jsonl\"\n        write_docs_jsonl(chunks, docs_path)\n        db = FaissFlatIP(dim=dim)\n        db.add(final_vecs)\n        db_path = out_root / \"faiss\" / \"index.faiss\"\n        db.save(str(db_path))\n        vectors_path = out_root / \"vectors.f16.bin\"\n        write_vectors_f16(final_vecs, vectors_path)\n\n        tags = _infer_tags(ext_counts)\n        _write_cpm_yml(\n            out_root,\n            name=out_root.name,\n            version=self.config.version,\n            description=source_path.as_posix(),\n            tags=tags,\n            entrypoints=[\"query\"],\n            embedding_model=self.config.model_name,\n            embedding_dim=dim,\n            embedding_normalized=True,\n        )\n\n        manifest = PacketManifest(\n            schema_version=\"1.0\",\n            packet_id=out_root.name,\n            embedding=EmbeddingSpec(\n                provider=\"sentence-transformers\",\n                model=self.config.model_name,\n                dim=dim,\n                dtype=\"float16\",\n                normalized=True,\n                max_seq_length=self.config.max_seq_length,\n            ),\n            similarity={\n                \"space\": \"cosine\",\n                \"index_type\": \"faiss.IndexFlatIP\",\n                \"notes\": \"cosine via inner product on normalized vectors\",\n            },\n            files={\n                \"docs\": \"docs.jsonl\",\n                \"vectors\": {\"path\": \"vectors.f16.bin\", \"format\": \"f16_rowmajor\"},\n                \"index\": {\"path\": \"faiss/index.faiss\", \"format\": \"faiss\"},\n                \"chunk_cache\": CHUNK_CACHE_NAME,\n                \"calibration\": None,\n            },\n            counts={\"docs\": len(chunks), \"vectors\": int(db.index.ntotal)},\n            source={\"input_dir\": source_path.as_posix(), \"file_ext_counts\": ext_counts},\n            cpm={\n                \"name\": out_root.name,\n                \"version\": self.config.version,\n                \"tags\": tags,\n                \"entrypoints\": [\"query\"],\n            },\n            incremental={\n                \"enabled\": bool(cache_pack or cache.files),\n                \"reused\": reused,\n                \"embedded\": len(to_embed_idx),\n                \"removed\": removed,\n                \"file_cache_hits\": file_cache_hits,\n                \"segment_cache_hits\": segment_cache_hits,\n                \"llm_calls\": llm_calls,\n            },\n        )\n        manifest.checksums = compute_checksums(\n            out_root,\n            [\"cpm.yml\", \"docs.jsonl\", \"vectors.f16.bin\", \"faiss/index.faiss\", CHUNK_CACHE_NAME],\n        )\n        manifest_path = out_root / \"manifest.json\"\n        write_manifest(manifest, manifest_path)\n\n        if self.config.archive:\n            archive_path = _archive_packet_dir(out_root, self.config.archive_format)\n            print(f\"[write] archive -> {archive_path}\")\n        print(\"[done] build ok\")\n        return manifest",
          "start": 205,
          "end": 527,
          "symbol": "build",
          "metadata": {}
        }
      ]
    },
    "cpm_llm_builder_plugin/llm_client.py": {
      "source_hash": "874c537a7194a30eff83dd5a119989d6b4fad53ff302c14f5be201095a8c7089",
      "classification": {
        "pipeline": "code_generic",
        "language": "python",
        "mime": "text/x-python"
      },
      "segments": [
        {
          "id": "cpm_llm_builder_plugin/llm_client.py:code_symbol:16:7ce4ede5ab",
          "kind": "code_symbol",
          "text": "class LLMClientConfig:\n    endpoint: str\n    model: str\n    request_timeout: float\n    prompt_version: str\n    max_retries: int = 2\n    retry_backoff_seconds: float = 0.5",
          "start": 16,
          "end": 24,
          "symbol": "LLMClientConfig",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/llm_client.py:code_symbol:25:03f65379d2",
          "kind": "code_symbol",
          "text": "def _prompt_text(prompt_version: str) -> str:\n    return (\n        \"You are a chunk enrichment model. \"\n        f\"Use prompt version {prompt_version}. \"\n        \"Return strict JSON with key 'chunks'. \"\n        \"For each input segment produce chunk fields: \"\n        \"id,title,summary,tags,anchors,text,relations.\"\n    )",
          "start": 25,
          "end": 34,
          "symbol": "_prompt_text",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/llm_client.py:code_symbol:35:ee4a6aed3d",
          "kind": "code_symbol",
          "text": "def _build_openai_like_payload(\n    *,\n    source: SourceDocument,\n    segments: Sequence[Segment],\n    constraints: ChunkConstraints,\n    model: str,\n    prompt_version: str,\n) -> dict[str, Any]:\n    return {\n        \"model\": model,\n        \"input\": [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": _prompt_text(prompt_version)},\n                    {\n                        \"type\": \"input_json\",\n                        \"json\": {\n                            \"task\": \"chunk.enrich\",\n                            \"source\": {\n                                \"path\": source.path,\n                                \"language\": source.language,\n                                \"mime\": source.mime,\n                                \"hash\": source.source_hash,\n                            },\n                            \"segments\": [segment.to_dict() for segment in segments],\n                            \"constraints\": constraints.to_dict(),\n                        },\n                    },\n                ],\n            }\n        ],\n        \"metadata\": {\n            \"cpm_plugin\": \"cpm-llm-builder\",\n            \"prompt_version\": prompt_version,\n        },\n    }",
          "start": 35,
          "end": 73,
          "symbol": "_build_openai_like_payload",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/llm_client.py:code_symbol:74:e82b6b58e3",
          "kind": "code_symbol",
          "text": "class LLMClient:",
          "start": 74,
          "end": 74,
          "symbol": "LLMClient",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/llm_client.py:code_symbol:75:b42e78e5e5",
          "kind": "code_symbol",
          "text": "def __init__(self, config: LLMClientConfig) -> None:\n        self.config = config",
          "start": 75,
          "end": 77,
          "symbol": "__init__",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/llm_client.py:code_symbol:78:16a2545156",
          "kind": "code_symbol",
          "text": "def enrich(\n        self,\n        *,\n        source: SourceDocument,\n        segments: Sequence[Segment],\n        constraints: ChunkConstraints,\n    ) -> list[Chunk]:\n        if not segments:\n            return []\n        payload = _build_openai_like_payload(\n            source=source,\n            segments=segments,\n            constraints=constraints,\n            model=self.config.model,\n            prompt_version=self.config.prompt_version,\n        )\n        last_exc: Exception | None = None\n        for attempt in range(self.config.max_retries + 1):\n            try:\n                response = requests.post(\n                    self.config.endpoint,\n                    json=payload,\n                    timeout=self.config.request_timeout,\n                )\n                response.raise_for_status()\n                return self._normalize_response(\n                    response.json(),\n                    segments=segments,\n                    source=source,\n                )\n            except Exception as exc:\n                last_exc = exc\n                if attempt >= self.config.max_retries:\n                    break\n                base = self.config.retry_backoff_seconds * (2**attempt)\n                jitter = random.uniform(0.0, 0.25)\n                time.sleep(base + jitter)\n        assert last_exc is not None\n        raise last_exc",
          "start": 78,
          "end": 117,
          "symbol": "enrich",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/llm_client.py:code_symbol:118:600a6c2ea2",
          "kind": "code_symbol",
          "text": "def _normalize_response(\n        self,\n        payload: Any,\n        *,\n        segments: Sequence[Segment],\n        source: SourceDocument,\n    ) -> list[Chunk]:\n        chunks = normalize_chunk_list(payload)\n        if not chunks:\n            raise ValueError(\"empty chunks in response\")\n        return _ensure_chunk_defaults(chunks, segments=segments, source=source)",
          "start": 118,
          "end": 130,
          "symbol": "_normalize_response",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/llm_client.py:code_symbol:131:ea37ec79aa",
          "kind": "code_symbol",
          "text": "def _ensure_chunk_defaults(chunks: Sequence[Chunk], *, segments: Sequence[Segment], source: SourceDocument) -> list[Chunk]:\n    index: dict[str, Segment] = {segment.id: segment for segment in segments}\n    resolved: list[Chunk] = []\n    for pos, chunk in enumerate(chunks):\n        segment = index.get(chunk.id) if chunk.id else None\n        if segment is None and pos < len(segments):\n            segment = segments[pos]\n        chunk_id = chunk.id or (segment.id if segment else f\"{source.path}:chunk:{pos}\")\n        anchors = dict(chunk.anchors)\n        if \"path\" not in anchors:\n            anchors[\"path\"] = source.path\n        if segment is not None:\n            anchors.setdefault(\"start_line\", segment.start_line)\n            anchors.setdefault(\"end_line\", segment.end_line)\n        text = chunk.text.strip() or (segment.text if segment else \"\")\n        resolved.append(\n            Chunk(\n                id=chunk_id,\n                text=text,\n                title=chunk.title,\n                summary=chunk.summary,\n                tags=chunk.tags,\n                anchors=anchors,\n                relations=dict(chunk.relations),\n                metadata=dict(chunk.metadata),\n            )\n        )\n    return resolved",
          "start": 131,
          "end": 159,
          "symbol": "_ensure_chunk_defaults",
          "metadata": {}
        }
      ]
    },
    "cpm_llm_builder_plugin/postprocess.py": {
      "source_hash": "73cb7f27703524a128fd84525fb430666a9348fddc6d1d9287e31ac025fc7dd5",
      "classification": {
        "pipeline": "code_generic",
        "language": "python",
        "mime": "text/x-python"
      },
      "segments": [
        {
          "id": "cpm_llm_builder_plugin/postprocess.py:code_symbol:10:0e7021dd26",
          "kind": "code_symbol",
          "text": "def _split_chunk(chunk: Chunk, max_tokens: int) -> list[Chunk]:\n    lines = chunk.text.splitlines()\n    if not lines:\n        return [chunk]\n    result: list[Chunk] = []\n    buffer: list[str] = []\n    part = 0\n    for line in lines:\n        candidate = \"\\n\".join(buffer + [line]).strip()\n        if buffer and estimate_tokens(candidate) > max_tokens:\n            text = \"\\n\".join(buffer).strip()\n            if text:\n                result.append(\n                    Chunk(\n                        id=f\"{chunk.id}:part:{part}\",\n                        text=text,\n                        title=chunk.title,\n                        summary=chunk.summary,\n                        tags=chunk.tags,\n                        anchors=dict(chunk.anchors),\n                        relations=dict(chunk.relations),\n                        metadata=dict(chunk.metadata),\n                    )\n                )\n                part += 1\n            buffer = [line]\n        else:\n            buffer.append(line)\n    final_text = \"\\n\".join(buffer).strip()\n    if final_text:\n        result.append(\n            Chunk(\n                id=f\"{chunk.id}:part:{part}\" if part else chunk.id,\n                text=final_text,\n                title=chunk.title,\n                summary=chunk.summary,\n                tags=chunk.tags,\n                anchors=dict(chunk.anchors),\n                relations=dict(chunk.relations),\n                metadata=dict(chunk.metadata),\n            )\n        )\n    return result or [chunk]",
          "start": 10,
          "end": 54,
          "symbol": "_split_chunk",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/postprocess.py:code_symbol:55:7d736914fc",
          "kind": "code_symbol",
          "text": "def _merge_small_chunks(chunks: Sequence[Chunk], min_tokens: int) -> list[Chunk]:\n    if not chunks:\n        return []\n    merged: list[Chunk] = []\n    buffer: Chunk | None = None\n    for chunk in chunks:\n        if buffer is None:\n            buffer = chunk\n            continue\n        if estimate_tokens(buffer.text) >= min_tokens:\n            merged.append(buffer)\n            buffer = chunk\n            continue\n        combined = f\"{buffer.text}\\n\\n{chunk.text}\".strip()\n        buffer = Chunk(\n            id=f\"{buffer.id}+{chunk.id}\",\n            text=combined,\n            title=buffer.title or chunk.title,\n            summary=buffer.summary or chunk.summary,\n            tags=tuple(sorted(set((*buffer.tags, *chunk.tags)))),\n            anchors=dict(buffer.anchors),\n            relations=dict(buffer.relations),\n            metadata=dict(buffer.metadata),\n        )\n    if buffer is not None:\n        merged.append(buffer)\n    return merged",
          "start": 55,
          "end": 83,
          "symbol": "_merge_small_chunks",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/postprocess.py:code_symbol:84:329718c009",
          "kind": "code_symbol",
          "text": "def apply_chunk_constraints(chunks: Sequence[Chunk], constraints: ChunkConstraints) -> list[Chunk]:\n    split_done: list[Chunk] = []\n    for chunk in chunks:\n        if estimate_tokens(chunk.text) > constraints.max_chunk_tokens:\n            split_done.extend(_split_chunk(chunk, constraints.max_chunk_tokens))\n        else:\n            split_done.append(chunk)\n    return _merge_small_chunks(split_done, constraints.min_chunk_tokens)",
          "start": 84,
          "end": 92,
          "symbol": "apply_chunk_constraints",
          "metadata": {}
        }
      ]
    },
    "cpm_llm_builder_plugin/prechunk.py": {
      "source_hash": "47a07331b7ecd007f9d554dc7210e18ad8ec5e5f596fe8991911106366dd758e",
      "classification": {
        "pipeline": "code_generic",
        "language": "python",
        "mime": "text/x-python"
      },
      "segments": [
        {
          "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:23:2d95072f11",
          "kind": "code_symbol",
          "text": "def _make_id(path: str, kind: str, start_line: int, text: str) -> str:\n    return f\"{path}:{kind}:{start_line}:{stable_hash(text)[:10]}\"",
          "start": 23,
          "end": 26,
          "symbol": "_make_id",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:27:dc8b0fb927",
          "kind": "code_symbol",
          "text": "def _segment(path: str, kind: str, text: str, start: int, end: int, symbol: str | None = None) -> Segment:\n    return Segment(\n        id=_make_id(path, kind, start, text),\n        kind=kind,\n        text=text.strip(),\n        start_line=start,\n        end_line=end,\n        symbol=symbol,\n        metadata={},\n    )",
          "start": 27,
          "end": 38,
          "symbol": "_segment",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:39:d54e87eb8d",
          "kind": "code_symbol",
          "text": "def _split_by_ranges(lines: list[str], ranges: Iterable[tuple[str, int, int, str | None]], path: str) -> list[Segment]:\n    segments: list[Segment] = []\n    for kind, start, end, symbol in ranges:\n        start_idx = max(start - 1, 0)\n        end_idx = min(end, len(lines))\n        text = \"\\n\".join(lines[start_idx:end_idx]).strip()\n        if not text:\n            continue\n        segments.append(_segment(path, kind, text, start, end, symbol))\n    return segments",
          "start": 39,
          "end": 50,
          "symbol": "_split_by_ranges",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:51:ba10266f03",
          "kind": "code_symbol",
          "text": "def _java_segments(path: str, content: str) -> list[Segment]:\n    lines = content.splitlines()\n    if not lines:\n        return []\n\n    ranges: list[tuple[str, int, int, str | None]] = []\n    current_start: int | None = None\n    current_kind = \"java_block\"\n    current_symbol: str | None = None\n    brace_depth = 0\n\n    for idx, line in enumerate(lines, start=1):\n        type_match = JAVA_TYPE_RE.match(line)\n        method_match = JAVA_METHOD_RE.match(line)\n        starts_symbol = type_match is not None or method_match is not None\n\n        if starts_symbol and current_start is None:\n            current_start = idx\n            if type_match is not None:\n                current_kind = \"class_header\"\n                current_symbol = type_match.group(3)\n            else:\n                current_kind = \"method\"\n                current_symbol = method_match.group(3) if method_match else None\n\n        brace_depth += line.count(\"{\")\n        brace_depth -= line.count(\"}\")\n\n        if current_start is not None and brace_depth <= 0:\n            ranges.append((current_kind, current_start, idx, current_symbol))\n            current_start = None\n            current_kind = \"java_block\"\n            current_symbol = None\n            brace_depth = 0\n\n    if current_start is not None:\n        ranges.append((current_kind, current_start, len(lines), current_symbol))\n\n    if not ranges:\n        ranges.append((\"java_file\", 1, len(lines), None))\n    return _split_by_ranges(lines, ranges, path)",
          "start": 51,
          "end": 93,
          "symbol": "_java_segments",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:94:44452949cc",
          "kind": "code_symbol",
          "text": "def _generic_code_segments(path: str, content: str) -> list[Segment]:\n    lines = content.splitlines()\n    if not lines:\n        return []\n    starts: list[tuple[int, str | None]] = []\n    for idx, line in enumerate(lines, start=1):\n        match = GENERIC_DEF_RE.match(line)\n        if match:\n            starts.append((idx, match.group(2)))\n    if not starts:\n        return [_segment(path, \"code_block\", content, 1, len(lines), None)]\n\n    ranges: list[tuple[str, int, int, str | None]] = []\n    for pos, (start, symbol) in enumerate(starts):\n        next_start = starts[pos + 1][0] if pos + 1 < len(starts) else len(lines) + 1\n        ranges.append((\"code_symbol\", start, next_start - 1, symbol))\n    return _split_by_ranges(lines, ranges, path)",
          "start": 94,
          "end": 112,
          "symbol": "_generic_code_segments",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:113:15857b676d",
          "kind": "code_symbol",
          "text": "def _markdown_segments(path: str, content: str) -> list[Segment]:\n    lines = content.splitlines()\n    if not lines:\n        return []\n    headings: list[tuple[int, str]] = []\n    for idx, line in enumerate(lines, start=1):\n        match = HEADING_RE.match(line)\n        if match:\n            headings.append((idx, match.group(2).strip()))\n    if not headings:\n        return [_segment(path, \"markdown_section\", content, 1, len(lines), None)]\n    ranges: list[tuple[str, int, int, str | None]] = []\n    for pos, (start, title) in enumerate(headings):\n        next_start = headings[pos + 1][0] if pos + 1 < len(headings) else len(lines) + 1\n        ranges.append((\"heading_section\", start, next_start - 1, title))\n    return _split_by_ranges(lines, ranges, path)",
          "start": 113,
          "end": 130,
          "symbol": "_markdown_segments",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:131:4fefa83a28",
          "kind": "code_symbol",
          "text": "def _json_yaml_segments(path: str, content: str, *, is_yaml: bool) -> list[Segment]:\n    try:\n        parsed = yaml.safe_load(content) if is_yaml else json.loads(content)\n    except Exception:\n        return [_segment(path, \"structured_blob\", content, 1, max(len(content.splitlines()), 1), None)]\n    lines = content.splitlines()\n    if not isinstance(parsed, dict):\n        return [_segment(path, \"structured_blob\", content, 1, max(len(lines), 1), None)]\n    if not lines:\n        return []\n    segments: list[Segment] = []\n    for key, value in parsed.items():\n        value_text = json.dumps(value, ensure_ascii=False, indent=2) if not is_yaml else yaml.safe_dump(value)\n        text = f\"{key}:\\n{value_text}\".strip()\n        segments.append(_segment(path, \"top_level_key\", text, 1, len(lines), str(key)))\n    return segments",
          "start": 131,
          "end": 148,
          "symbol": "_json_yaml_segments",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:149:b105bb6cbf",
          "kind": "code_symbol",
          "text": "def _text_segments(path: str, content: str) -> list[Segment]:\n    parts = [part.strip() for part in re.split(r\"\\n\\s*\\n\", content) if part.strip()]\n    if not parts:\n        return []\n    lines = content.splitlines()\n    segments: list[Segment] = []\n    cursor = 1\n    for idx, part in enumerate(parts):\n        part_lines = part.splitlines()\n        line_count = max(len(part_lines), 1)\n        start = cursor\n        end = min(cursor + line_count - 1, max(len(lines), 1))\n        cursor = end + 1\n        segments.append(_segment(path, \"paragraph\", part, start, end, f\"paragraph_{idx+1}\"))\n    return segments",
          "start": 149,
          "end": 165,
          "symbol": "_text_segments",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:166:44982827aa",
          "kind": "code_symbol",
          "text": "def prechunk(path: str, content: str, classification: FileClassification) -> list[Segment]:\n    pipeline = classification.pipeline\n    if pipeline == \"java\":\n        return _java_segments(path, content)\n    if pipeline == \"code_generic\":\n        return _generic_code_segments(path, content)\n    if pipeline in {\"markdown\", \"html\"}:\n        return _markdown_segments(path, content)\n    if pipeline == \"json\":\n        return _json_yaml_segments(path, content, is_yaml=False)\n    if pipeline == \"yaml\":\n        return _json_yaml_segments(path, content, is_yaml=True)\n    return _text_segments(path, content)",
          "start": 166,
          "end": 179,
          "symbol": "prechunk",
          "metadata": {}
        }
      ]
    },
    "cpm_llm_builder_plugin/prompts/chunk_enrich_v1.txt": {
      "source_hash": "18ec58d5013a25c65ec3ec40323aa969cf0fac40332b0e70448e9acf116a7dff",
      "classification": {
        "pipeline": "text",
        "language": "text",
        "mime": "text/plain"
      },
      "segments": [
        {
          "id": "cpm_llm_builder_plugin/prompts/chunk_enrich_v1.txt:paragraph:1:bf44f5a00a",
          "kind": "paragraph",
          "text": "You are a chunk enrichment model.\nGiven source metadata and deterministic segments, return JSON object with key \"chunks\".\nFor each segment return:\n- id\n- title\n- summary\n- tags (array of short tags)\n- anchors (path, start_line, end_line)\n- text (can be original segment text)\n- relations (calls, called_by, related_symbols if available)",
          "start": 1,
          "end": 10,
          "symbol": "paragraph_1",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/prompts/chunk_enrich_v1.txt:paragraph:11:603337ad03",
          "kind": "paragraph",
          "text": "Never omit segments. Keep output deterministic and concise.",
          "start": 11,
          "end": 11,
          "symbol": "paragraph_2",
          "metadata": {}
        }
      ]
    },
    "cpm_llm_builder_plugin/schemas.py": {
      "source_hash": "fd94e416d3baba8ee42552267d67a72ea7225018a07add3d5171ed44dd85c869",
      "classification": {
        "pipeline": "code_generic",
        "language": "python",
        "mime": "text/x-python"
      },
      "segments": [
        {
          "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:11:7e0858321b",
          "kind": "code_symbol",
          "text": "def estimate_tokens(text: str) -> int:\n    \"\"\"Very rough token estimator used for local constraints.\"\"\"\n\n    cleaned = text.strip()\n    if not cleaned:\n        return 0\n    return max(1, len(cleaned) // 4)",
          "start": 11,
          "end": 19,
          "symbol": "estimate_tokens",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:20:009f9971ca",
          "kind": "code_symbol",
          "text": "def stable_hash(payload: str) -> str:\n    return hashlib.sha256(payload.encode(\"utf-8\")).hexdigest()\n\n\n@dataclass(frozen=True)",
          "start": 20,
          "end": 24,
          "symbol": "stable_hash",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:25:14be8d3391",
          "kind": "code_symbol",
          "text": "class SourceDocument:\n    path: str\n    language: str\n    mime: str\n    source_hash: str\n\n\n@dataclass(frozen=True)",
          "start": 25,
          "end": 32,
          "symbol": "SourceDocument",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:33:e9005cb48d",
          "kind": "code_symbol",
          "text": "class Segment:\n    id: str\n    kind: str\n    text: str\n    start_line: int\n    end_line: int\n    symbol: str | None = None\n    metadata: dict[str, Any] = field(default_factory=dict)",
          "start": 33,
          "end": 41,
          "symbol": "Segment",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:42:77fe66d391",
          "kind": "code_symbol",
          "text": "def to_dict(self) -> dict[str, Any]:\n        return {\n            \"id\": self.id,\n            \"kind\": self.kind,\n            \"text\": self.text,\n            \"start\": self.start_line,\n            \"end\": self.end_line,\n            \"symbol\": self.symbol,\n            \"metadata\": dict(self.metadata),\n        }\n\n    @classmethod",
          "start": 42,
          "end": 53,
          "symbol": "to_dict",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:54:e09f116b8d",
          "kind": "code_symbol",
          "text": "def from_dict(cls, payload: Mapping[str, Any]) -> \"Segment\":\n        return cls(\n            id=str(payload[\"id\"]),\n            kind=str(payload.get(\"kind\") or \"segment\"),\n            text=str(payload.get(\"text\") or \"\"),\n            start_line=int(payload.get(\"start\", 1)),\n            end_line=int(payload.get(\"end\", payload.get(\"start\", 1))),\n            symbol=str(payload[\"symbol\"]) if payload.get(\"symbol\") else None,\n            metadata=dict(payload.get(\"metadata\") or {}),\n        )\n\n\n@dataclass(frozen=True)",
          "start": 54,
          "end": 66,
          "symbol": "from_dict",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:67:5799810f80",
          "kind": "code_symbol",
          "text": "class Chunk:\n    id: str\n    text: str\n    title: str = \"\"\n    summary: str = \"\"\n    tags: tuple[str, ...] = ()\n    anchors: dict[str, Any] = field(default_factory=dict)\n    relations: dict[str, Any] = field(default_factory=dict)\n    metadata: dict[str, Any] = field(default_factory=dict)",
          "start": 67,
          "end": 76,
          "symbol": "Chunk",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:77:5abb8d6aae",
          "kind": "code_symbol",
          "text": "def to_dict(self) -> dict[str, Any]:\n        return {\n            \"id\": self.id,\n            \"title\": self.title,\n            \"summary\": self.summary,\n            \"tags\": list(self.tags),\n            \"anchors\": dict(self.anchors),\n            \"text\": self.text,\n            \"relations\": dict(self.relations),\n            \"metadata\": dict(self.metadata),\n        }\n\n    @classmethod",
          "start": 77,
          "end": 89,
          "symbol": "to_dict",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:90:6d29319295",
          "kind": "code_symbol",
          "text": "def from_dict(cls, payload: Mapping[str, Any]) -> \"Chunk\":\n        tags_raw = payload.get(\"tags\") or []\n        tags = tuple(str(item) for item in tags_raw if isinstance(item, str))\n        return cls(\n            id=str(payload.get(\"id\") or \"\"),\n            text=str(payload.get(\"text\") or \"\"),\n            title=str(payload.get(\"title\") or \"\"),\n            summary=str(payload.get(\"summary\") or \"\"),\n            tags=tags,\n            anchors=dict(payload.get(\"anchors\") or {}),\n            relations=dict(payload.get(\"relations\") or {}),\n            metadata=dict(payload.get(\"metadata\") or {}),\n        )\n\n\n@dataclass(frozen=True)",
          "start": 90,
          "end": 105,
          "symbol": "from_dict",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:106:d00d89d3db",
          "kind": "code_symbol",
          "text": "class ChunkConstraints:\n    max_chunk_tokens: int = 800\n    min_chunk_tokens: int = 120\n    max_segments_per_request: int = 8",
          "start": 106,
          "end": 110,
          "symbol": "ChunkConstraints",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:111:7a642b2975",
          "kind": "code_symbol",
          "text": "def to_dict(self) -> dict[str, int]:\n        return {\n            \"max_chunk_tokens\": int(self.max_chunk_tokens),\n            \"min_chunk_tokens\": int(self.min_chunk_tokens),\n            \"max_segments_per_request\": int(self.max_segments_per_request),\n        }",
          "start": 111,
          "end": 118,
          "symbol": "to_dict",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:119:19c7420606",
          "kind": "code_symbol",
          "text": "def segment_cache_key(\n    *,\n    segment: Segment,\n    model: str,\n    prompt_version: str,\n    constraints: ChunkConstraints,\n) -> str:\n    payload = {\n        \"segment\": segment.to_dict(),\n        \"model\": model,\n        \"prompt_version\": prompt_version,\n        \"constraints\": constraints.to_dict(),\n    }\n    return stable_hash(json.dumps(payload, ensure_ascii=False, sort_keys=True))",
          "start": 119,
          "end": 134,
          "symbol": "segment_cache_key",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:135:fa6df14b44",
          "kind": "code_symbol",
          "text": "def normalize_chunk_list(payload: Any) -> list[Chunk]:\n    \"\"\"Convert legacy/OpenAI-like payloads to chunks.\"\"\"\n\n    raw_chunks: Any = None\n\n    if isinstance(payload, list):\n        raw_chunks = payload\n    elif isinstance(payload, Mapping):\n        if \"chunks\" in payload:\n            raw_chunks = payload.get(\"chunks\")\n        elif \"output\" in payload:\n            output = payload.get(\"output\")\n            if isinstance(output, Sequence):\n                for item in output:\n                    if not isinstance(item, Mapping):\n                        continue\n                    if item.get(\"type\") != \"output_json\":\n                        continue\n                    json_payload = item.get(\"json\")\n                    if isinstance(json_payload, Mapping) and \"chunks\" in json_payload:\n                        raw_chunks = json_payload.get(\"chunks\")\n                        break\n\n    if not isinstance(raw_chunks, list):\n        raise ValueError(\"LLM response does not contain chunk list\")\n\n    chunks: list[Chunk] = []\n    for item in raw_chunks:\n        if isinstance(item, str):\n            text = item.strip()\n            if not text:\n                continue\n            chunks.append(Chunk(id=\"\", text=text))\n            continue\n        if isinstance(item, Mapping):\n            chunk = Chunk.from_dict(item)\n            if chunk.text.strip():\n                chunks.append(chunk)\n    return chunks",
          "start": 135,
          "end": 174,
          "symbol": "normalize_chunk_list",
          "metadata": {}
        }
      ]
    },
    "cpm_llm_builder_plugin/validators.py": {
      "source_hash": "86979c89f1ee771573571d8aa034d556af77b0a848b32b6f0918b6ceb7b97906",
      "classification": {
        "pipeline": "code_generic",
        "language": "python",
        "mime": "text/x-python"
      },
      "segments": [
        {
          "id": "cpm_llm_builder_plugin/validators.py:code_symbol:12:0e442e8845",
          "kind": "code_symbol",
          "text": "class ValidationResult:\n    chunks: tuple[Chunk, ...]\n    warnings: tuple[str, ...]",
          "start": 12,
          "end": 16,
          "symbol": "ValidationResult",
          "metadata": {}
        },
        {
          "id": "cpm_llm_builder_plugin/validators.py:code_symbol:17:2354e282c3",
          "kind": "code_symbol",
          "text": "def validate_chunks(chunks: Sequence[Chunk]) -> ValidationResult:\n    warnings: list[str] = []\n    seen: set[str] = set()\n    valid: list[Chunk] = []\n\n    for chunk in chunks:\n        if not chunk.text.strip():\n            warnings.append(f\"chunk {chunk.id!r} dropped: empty text\")\n            continue\n        if not chunk.id:\n            warnings.append(\"chunk dropped: missing id\")\n            continue\n        if chunk.id in seen:\n            warnings.append(f\"chunk {chunk.id!r} dropped: duplicate id\")\n            continue\n        seen.add(chunk.id)\n        anchors = dict(chunk.anchors)\n        if \"path\" not in anchors:\n            warnings.append(f\"chunk {chunk.id!r} has no anchors.path\")\n        if not chunk.summary:\n            warnings.append(f\"chunk {chunk.id!r} has empty summary\")\n        if not chunk.tags:\n            warnings.append(f\"chunk {chunk.id!r} has empty tags\")\n        valid.append(chunk)\n\n    return ValidationResult(chunks=tuple(valid), warnings=tuple(warnings))",
          "start": 17,
          "end": 43,
          "symbol": "validate_chunks",
          "metadata": {}
        }
      ]
    }
  },
  "segment_enrichment": {
    "035e87d2033aed323c5e4f5052d1aacd760e3f642f15397ef56b8f78a2f9e5d7": {
      "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:33:e9005cb48d",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/schemas.py",
        "start_line": 33,
        "end_line": 41
      },
      "text": "class Segment:\n    id: str\n    kind: str\n    text: str\n    start_line: int\n    end_line: int\n    symbol: str | None = None\n    metadata: dict[str, Any] = field(default_factory=dict)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "0786c65a75403639334ee7a73d18024bfc610463cf64ce2a9bc4222e4adda32f": {
      "id": "DESIGN.md:heading_section:16:a1898e947b",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "DESIGN.md",
        "start_line": 16,
        "end_line": 22
      },
      "text": "## Compatibility\n\n- Supports legacy chunk responses:\n  - `[\"chunk1\", \"chunk2\"]`\n  - `{\"chunks\": [...]}`\n- Supports OpenAI-like envelopes with `output_json`.",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "07af00071cb914ea36d8d90747505ef9f3715f04a3f1c12ff6b332e57fdfa8e6": {
      "id": "cpm_llm_builder_plugin/cache.py:code_symbol:17:87726a0eb5",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/cache.py",
        "start_line": 17,
        "end_line": 23
      },
      "text": "class FileCacheEntry:\n    source_hash: str\n    classification: dict[str, Any] = field(default_factory=dict)\n    segments: list[Segment] = field(default_factory=list)\n\n\n@dataclass",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "0a0f7d01460d3e8927e138ba6d27c1f199d6502d0aa94237b3094e5dd7886eca": {
      "id": "cpm_llm_builder_plugin/postprocess.py:code_symbol:55:7d736914fc",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/postprocess.py",
        "start_line": 55,
        "end_line": 83
      },
      "text": "def _merge_small_chunks(chunks: Sequence[Chunk], min_tokens: int) -> list[Chunk]:\n    if not chunks:\n        return []\n    merged: list[Chunk] = []\n    buffer: Chunk | None = None\n    for chunk in chunks:\n        if buffer is None:\n            buffer = chunk\n            continue\n        if estimate_tokens(buffer.text) >= min_tokens:\n            merged.append(buffer)\n            buffer = chunk\n            continue\n        combined = f\"{buffer.text}\\n\\n{chunk.text}\".strip()\n        buffer = Chunk(\n            id=f\"{buffer.id}+{chunk.id}\",\n            text=combined,\n            title=buffer.title or chunk.title,\n            summary=buffer.summary or chunk.summary,\n            tags=tuple(sorted(set((*buffer.tags, *chunk.tags)))),\n            anchors=dict(buffer.anchors),\n            relations=dict(buffer.relations),\n            metadata=dict(buffer.metadata),\n        )\n    if buffer is not None:\n        merged.append(buffer)\n    return merged",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "1a8f842dee8fd882260f8db9568fae6b33fc2d70b79bf309428767cbf09678d9": {
      "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:23:2d95072f11",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/prechunk.py",
        "start_line": 23,
        "end_line": 26
      },
      "text": "def _make_id(path: str, kind: str, start_line: int, text: str) -> str:\n    return f\"{path}:{kind}:{start_line}:{stable_hash(text)[:10]}\"",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "24f7b02db310fa044b37238834d2dc507bbd96c8467eb5acf6123de94453e24c": {
      "id": "cpm_llm_builder_plugin/validators.py:code_symbol:17:2354e282c3",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/validators.py",
        "start_line": 17,
        "end_line": 43
      },
      "text": "def validate_chunks(chunks: Sequence[Chunk]) -> ValidationResult:\n    warnings: list[str] = []\n    seen: set[str] = set()\n    valid: list[Chunk] = []\n\n    for chunk in chunks:\n        if not chunk.text.strip():\n            warnings.append(f\"chunk {chunk.id!r} dropped: empty text\")\n            continue\n        if not chunk.id:\n            warnings.append(\"chunk dropped: missing id\")\n            continue\n        if chunk.id in seen:\n            warnings.append(f\"chunk {chunk.id!r} dropped: duplicate id\")\n            continue\n        seen.add(chunk.id)\n        anchors = dict(chunk.anchors)\n        if \"path\" not in anchors:\n            warnings.append(f\"chunk {chunk.id!r} has no anchors.path\")\n        if not chunk.summary:\n            warnings.append(f\"chunk {chunk.id!r} has empty summary\")\n        if not chunk.tags:\n            warnings.append(f\"chunk {chunk.id!r} has empty tags\")\n        valid.append(chunk)\n\n    return ValidationResult(chunks=tuple(valid), warnings=tuple(warnings))",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "2694cef40f66a2b6d8e1f654f2a340287e5fc39ffadcc1ff7b09aa9df46fd414": {
      "id": "cpm_llm_builder_plugin/llm_client.py:code_symbol:78:16a2545156",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/llm_client.py",
        "start_line": 78,
        "end_line": 117
      },
      "text": "def enrich(\n        self,\n        *,\n        source: SourceDocument,\n        segments: Sequence[Segment],\n        constraints: ChunkConstraints,\n    ) -> list[Chunk]:\n        if not segments:\n            return []\n        payload = _build_openai_like_payload(\n            source=source,\n            segments=segments,\n            constraints=constraints,\n            model=self.config.model,\n            prompt_version=self.config.prompt_version,\n        )\n        last_exc: Exception | None = None\n        for attempt in range(self.config.max_retries + 1):\n            try:\n                response = requests.post(\n                    self.config.endpoint,\n                    json=payload,\n                    timeout=self.config.request_timeout,\n                )\n                response.raise_for_status()\n                return self._normalize_response(\n                    response.json(),\n                    segments=segments,\n                    source=source,\n                )\n            except Exception as exc:\n                last_exc = exc\n                if attempt >= self.config.max_retries:\n                    break\n                base = self.config.retry_backoff_seconds * (2**attempt)\n                jitter = random.uniform(0.0, 0.25)\n                time.sleep(base + jitter)\n        assert last_exc is not None\n        raise last_exc",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "285e0f0d4fd594799162fa2f9c8897c09ae767db92b67bca37ebe15e2d94e227": {
      "id": "cpm_llm_builder_plugin/llm_client.py:code_symbol:75:b42e78e5e5",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/llm_client.py",
        "start_line": 75,
        "end_line": 77
      },
      "text": "def __init__(self, config: LLMClientConfig) -> None:\n        self.config = config",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "3115428d3d9216aed0a3d6fb944a295a06f3141016fef3a2460f40ef90a3e009": {
      "id": "cpm_llm_builder_plugin/features.py:code_symbol:119:002670001a",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/features.py",
        "start_line": 119,
        "end_line": 120
      },
      "text": "class CPMLLMBuilder(CPMAbstractBuilder):\n    @classmethod",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "396832662f226ffbac735e2379e5310e9b72896c7c10d7e9ca626fbfef6080fb": {
      "id": "cpm_llm_builder_plugin/llm_client.py:code_symbol:118:600a6c2ea2",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/llm_client.py",
        "start_line": 118,
        "end_line": 130
      },
      "text": "def _normalize_response(\n        self,\n        payload: Any,\n        *,\n        segments: Sequence[Segment],\n        source: SourceDocument,\n    ) -> list[Chunk]:\n        chunks = normalize_chunk_list(payload)\n        if not chunks:\n            raise ValueError(\"empty chunks in response\")\n        return _ensure_chunk_defaults(chunks, segments=segments, source=source)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "3ccc42365b98e88e7a9db6362e7d88b7f05095dd19b5c215a2f13e0cce11a811": {
      "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:11:7e0858321b",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/schemas.py",
        "start_line": 11,
        "end_line": 19
      },
      "text": "def estimate_tokens(text: str) -> int:\n    \"\"\"Very rough token estimator used for local constraints.\"\"\"\n\n    cleaned = text.strip()\n    if not cleaned:\n        return 0\n    return max(1, len(cleaned) // 4)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "4021fae8fd9c6251d1dfdc75894fa13a04e7798f0e76afea248d71cac539433d": {
      "id": "config.yml:top_level_key:1:a5a2cb6f08",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "config.yml",
        "start_line": 1,
        "end_line": 12
      },
      "text": "request_timeout:\n30.0\n...",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "42afd31091bd8e41dc14e2441d2a2d07f3bf5ee7a53544e20d2ed7819023c229": {
      "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:67:5799810f80",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/schemas.py",
        "start_line": 67,
        "end_line": 76
      },
      "text": "class Chunk:\n    id: str\n    text: str\n    title: str = \"\"\n    summary: str = \"\"\n    tags: tuple[str, ...] = ()\n    anchors: dict[str, Any] = field(default_factory=dict)\n    relations: dict[str, Any] = field(default_factory=dict)\n    metadata: dict[str, Any] = field(default_factory=dict)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "439b4326c157bc1f49fb51f3486f228246174b260f98dd136874429f3335ef41": {
      "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:25:14be8d3391",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/schemas.py",
        "start_line": 25,
        "end_line": 32
      },
      "text": "class SourceDocument:\n    path: str\n    language: str\n    mime: str\n    source_hash: str\n\n\n@dataclass(frozen=True)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "44fc594f054ba51f194c54539cb2954e2b9c787ae23a01c02a82af55fc3b1b54": {
      "id": "cpm_llm_builder_plugin/features.py:code_symbol:101:821fa67b67",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/features.py",
        "start_line": 101,
        "end_line": 118
      },
      "text": "class LLMBuilderRuntimeConfig:\n    llm_endpoint: str\n    request_timeout: float\n    llm_model: str\n    prompt_version: str\n    max_retries: int\n    constraints: ChunkConstraints\n    model_name: str\n    max_seq_length: int\n    version: str\n    archive: bool\n    archive_format: str\n    embed_url: str\n    embeddings_mode: str\n    timeout: float | None\n\n\n@cpmbuilder(name=\"cpm-llm-builder\", group=\"llm\")",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "480772b4291eb34be23bbb7e48b4f6127f8ba1bdb949351c93359e83ed3e89b9": {
      "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:111:7a642b2975",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/schemas.py",
        "start_line": 111,
        "end_line": 118
      },
      "text": "def to_dict(self) -> dict[str, int]:\n        return {\n            \"max_chunk_tokens\": int(self.max_chunk_tokens),\n            \"min_chunk_tokens\": int(self.min_chunk_tokens),\n            \"max_segments_per_request\": int(self.max_segments_per_request),\n        }",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "5086923cf75c72ab11f3a91e56c1488c8579ae99bf8872007cba720bd34fb82f": {
      "id": "cpm_llm_builder_plugin/cache.py:code_symbol:115:ac27c9054f",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/cache.py",
        "start_line": 115,
        "end_line": 132
      },
      "text": "def save_cache(path: Path, cache: CacheV2) -> None:\n    payload = {\n        \"version\": CACHE_VERSION,\n        \"files\": {\n            rel: {\n                \"source_hash\": entry.source_hash,\n                \"classification\": dict(entry.classification),\n                \"segments\": [segment.to_dict() for segment in entry.segments],\n            }\n            for rel, entry in sorted(cache.files.items())\n        },\n        \"segment_enrichment\": {\n            key: value.to_dict()\n            for key, value in sorted(cache.segment_enrichment.items())\n        },\n    }\n    path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding=\"utf-8\")",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "574ada3bb75d12bfb69e32687b43bf9a0fbf63f88c4755e2aade08731c9dcac8": {
      "id": "cpm_llm_builder_plugin/postprocess.py:code_symbol:84:329718c009",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/postprocess.py",
        "start_line": 84,
        "end_line": 92
      },
      "text": "def apply_chunk_constraints(chunks: Sequence[Chunk], constraints: ChunkConstraints) -> list[Chunk]:\n    split_done: list[Chunk] = []\n    for chunk in chunks:\n        if estimate_tokens(chunk.text) > constraints.max_chunk_tokens:\n            split_done.extend(_split_chunk(chunk, constraints.max_chunk_tokens))\n        else:\n            split_done.append(chunk)\n    return _merge_small_chunks(split_done, constraints.min_chunk_tokens)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "5d7066d2934f79cb1f2f84111ac1b8d70df3febe318fc34296e17772cb8fb9ab": {
      "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:119:19c7420606",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/schemas.py",
        "start_line": 119,
        "end_line": 134
      },
      "text": "def segment_cache_key(\n    *,\n    segment: Segment,\n    model: str,\n    prompt_version: str,\n    constraints: ChunkConstraints,\n) -> str:\n    payload = {\n        \"segment\": segment.to_dict(),\n        \"model\": model,\n        \"prompt_version\": prompt_version,\n        \"constraints\": constraints.to_dict(),\n    }\n    return stable_hash(json.dumps(payload, ensure_ascii=False, sort_keys=True))",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "63434f52417190950f96bd4bdf198d6374549d1f70e87380d4f905efcad4577b": {
      "id": "README.md:heading_section:62:a8844e60da",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "README.md",
        "start_line": 62,
        "end_line": 78
      },
      "text": "## Config\n\n`config.yml`:\n\n```yaml\nllm:\n  endpoint: \"http://127.0.0.1:9000/chunk\"\n  model: \"chunker-xxx\"\n  prompt_version: \"chunk_enrich_v1\"\n  max_retries: 2\nrequest_timeout: 30.0\nconstraints:\n  max_chunk_tokens: 800\n  min_chunk_tokens: 120\n  max_segments_per_request: 8\n```",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "64667b1a4c4fff3b34d616190ba4d23af713cf4f861bcd10ae9000495e999c04": {
      "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:20:009f9971ca",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/schemas.py",
        "start_line": 20,
        "end_line": 24
      },
      "text": "def stable_hash(payload: str) -> str:\n    return hashlib.sha256(payload.encode(\"utf-8\")).hexdigest()\n\n\n@dataclass(frozen=True)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "6673ea9b26e30fde6c2bf38c3d350c96311f4fa743592b474e52c51c7c3b5381": {
      "id": "cpm_llm_builder_plugin/features.py:code_symbol:144:33d424f6c2",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/features.py",
        "start_line": 144,
        "end_line": 147
      },
      "text": "def __init__(self, config: LLMBuilderRuntimeConfig | None = None, *, embedder: Any | None = None) -> None:\n        self.config = config\n        self.embedder = embedder",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "6bcfbf73ccac32ef4677795433889343abcf313df4a8ca0efc3531d3fe98ba34": {
      "id": "cpm_llm_builder_plugin/llm_client.py:code_symbol:35:ee4a6aed3d",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/llm_client.py",
        "start_line": 35,
        "end_line": 73
      },
      "text": "def _build_openai_like_payload(\n    *,\n    source: SourceDocument,\n    segments: Sequence[Segment],\n    constraints: ChunkConstraints,\n    model: str,\n    prompt_version: str,\n) -> dict[str, Any]:\n    return {\n        \"model\": model,\n        \"input\": [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": _prompt_text(prompt_version)},\n                    {\n                        \"type\": \"input_json\",\n                        \"json\": {\n                            \"task\": \"chunk.enrich\",\n                            \"source\": {\n                                \"path\": source.path,\n                                \"language\": source.language,\n                                \"mime\": source.mime,\n                                \"hash\": source.source_hash,\n                            },\n                            \"segments\": [segment.to_dict() for segment in segments],\n                            \"constraints\": constraints.to_dict(),\n                        },\n                    },\n                ],\n            }\n        ],\n        \"metadata\": {\n            \"cpm_plugin\": \"cpm-llm-builder\",\n            \"prompt_version\": prompt_version,\n        },\n    }",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "72799c7057e4c1cf3e0c7c20837c24e267af71b9842009bf9d9e1bf0f1042150": {
      "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:166:44982827aa",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/prechunk.py",
        "start_line": 166,
        "end_line": 179
      },
      "text": "def prechunk(path: str, content: str, classification: FileClassification) -> list[Segment]:\n    pipeline = classification.pipeline\n    if pipeline == \"java\":\n        return _java_segments(path, content)\n    if pipeline == \"code_generic\":\n        return _generic_code_segments(path, content)\n    if pipeline in {\"markdown\", \"html\"}:\n        return _markdown_segments(path, content)\n    if pipeline == \"json\":\n        return _json_yaml_segments(path, content, is_yaml=False)\n    if pipeline == \"yaml\":\n        return _json_yaml_segments(path, content, is_yaml=True)\n    return _text_segments(path, content)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "72ec18c08e617c31c2823bc71198328d5cc2d3f5c1226f8e62c6d60bc958313c": {
      "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:77:5abb8d6aae",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/schemas.py",
        "start_line": 77,
        "end_line": 89
      },
      "text": "def to_dict(self) -> dict[str, Any]:\n        return {\n            \"id\": self.id,\n            \"title\": self.title,\n            \"summary\": self.summary,\n            \"tags\": list(self.tags),\n            \"anchors\": dict(self.anchors),\n            \"text\": self.text,\n            \"relations\": dict(self.relations),\n            \"metadata\": dict(self.metadata),\n        }\n\n    @classmethod",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "72f28446687bcc7cd51411efc1738dab8a75cc416e800a699cd19846c778f142": {
      "id": "cpm_llm_builder_plugin/features.py:code_symbol:46:6ab4ccacc6",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/features.py",
        "start_line": 46,
        "end_line": 50
      },
      "text": "def set_plugin_root(path: Path) -> None:\n    global _PLUGIN_ROOT\n    _PLUGIN_ROOT = path",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "743edab2d23f7497e61963d8262421aa6728b2cf4d2da5fb0d5d7c03d3fb7015": {
      "id": "cpm_llm_builder_plugin/prompts/chunk_enrich_v1.txt:paragraph:11:603337ad03",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/prompts/chunk_enrich_v1.txt",
        "start_line": 11,
        "end_line": 11
      },
      "text": "Never omit segments. Keep output deterministic and concise.",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "75511009289430c280eaa769b28f61ebc34b4631a2f64574c400d5a80b11c4f1": {
      "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:94:44452949cc",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/prechunk.py",
        "start_line": 94,
        "end_line": 112
      },
      "text": "def _generic_code_segments(path: str, content: str) -> list[Segment]:\n    lines = content.splitlines()\n    if not lines:\n        return []\n    starts: list[tuple[int, str | None]] = []\n    for idx, line in enumerate(lines, start=1):\n        match = GENERIC_DEF_RE.match(line)\n        if match:\n            starts.append((idx, match.group(2)))\n    if not starts:\n        return [_segment(path, \"code_block\", content, 1, len(lines), None)]\n\n    ranges: list[tuple[str, int, int, str | None]] = []\n    for pos, (start, symbol) in enumerate(starts):\n        next_start = starts[pos + 1][0] if pos + 1 < len(starts) else len(lines) + 1\n        ranges.append((\"code_symbol\", start, next_start - 1, symbol))\n    return _split_by_ranges(lines, ranges, path)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "7f4d653c7b7ec558a633c48421cb4682bc57e8a84764b33d3602d2adc4657025": {
      "id": "DESIGN.md:heading_section:1:1a9c53297d",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "DESIGN.md",
        "start_line": 1,
        "end_line": 2
      },
      "text": "# cpm-llm-builder Design Notes",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "888be0babdb1fa3520fb3566b512c048455946fcc21015f7956977c30ae5334b": {
      "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:149:b105bb6cbf",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/prechunk.py",
        "start_line": 149,
        "end_line": 165
      },
      "text": "def _text_segments(path: str, content: str) -> list[Segment]:\n    parts = [part.strip() for part in re.split(r\"\\n\\s*\\n\", content) if part.strip()]\n    if not parts:\n        return []\n    lines = content.splitlines()\n    segments: list[Segment] = []\n    cursor = 1\n    for idx, part in enumerate(parts):\n        part_lines = part.splitlines()\n        line_count = max(len(part_lines), 1)\n        start = cursor\n        end = min(cursor + line_count - 1, max(len(lines), 1))\n        cursor = end + 1\n        segments.append(_segment(path, \"paragraph\", part, start, end, f\"paragraph_{idx+1}\"))\n    return segments",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "91f1cc8f0535389b3f0f004a522168490124a4a1500eff8eafe98d9898b7430f": {
      "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:113:15857b676d",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/prechunk.py",
        "start_line": 113,
        "end_line": 130
      },
      "text": "def _markdown_segments(path: str, content: str) -> list[Segment]:\n    lines = content.splitlines()\n    if not lines:\n        return []\n    headings: list[tuple[int, str]] = []\n    for idx, line in enumerate(lines, start=1):\n        match = HEADING_RE.match(line)\n        if match:\n            headings.append((idx, match.group(2).strip()))\n    if not headings:\n        return [_segment(path, \"markdown_section\", content, 1, len(lines), None)]\n    ranges: list[tuple[str, int, int, str | None]] = []\n    for pos, (start, title) in enumerate(headings):\n        next_start = headings[pos + 1][0] if pos + 1 < len(headings) else len(lines) + 1\n        ranges.append((\"heading_section\", start, next_start - 1, title))\n    return _split_by_ranges(lines, ranges, path)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "93b3bd72b05277725230615c5692fc22d49717c58cffc013ebaf8c660d262dfd": {
      "id": "cpm_llm_builder_plugin/features.py:code_symbol:55:fcdb002f55",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/features.py",
        "start_line": 55,
        "end_line": 63
      },
      "text": "def _resolve_config_path(config_arg: str | None) -> Path:\n    if config_arg:\n        return Path(config_arg).expanduser().resolve()\n    if _PLUGIN_ROOT is not None:\n        return (_PLUGIN_ROOT / DEFAULT_CONFIG_NAME).resolve()\n    return Path(DEFAULT_CONFIG_NAME).resolve()\n\n\n@dataclass(frozen=True)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "a351d3f087599526041f8399eae725a06f99369e8f998acda2782db5f9124880": {
      "id": "cpm_llm_builder_plugin/classifiers.py:code_symbol:42:ea72afdcb9",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/classifiers.py",
        "start_line": 42,
        "end_line": 64
      },
      "text": "def classify_file(path: Path, content: str) -> FileClassification:\n    ext = path.suffix.lower()\n    known = PIPELINES_BY_EXT.get(ext)\n    if known is not None:\n        return known\n\n    head = content.lstrip()[:128]\n    if head.startswith(\"#!\"):\n        if \"python\" in head:\n            return FileClassification(\"code_generic\", \"python\", \"text/x-python\")\n        if \"bash\" in head or \"sh\" in head:\n            return FileClassification(\"text\", \"shell\", \"text/x-shellscript\")\n\n    if \"\\x00\" in content:\n        return FileClassification(\n            pipeline=\"binary_unsupported\",\n            language=\"binary\",\n            mime=\"application/octet-stream\",\n            is_supported_text=False,\n        )\n\n    return FileClassification(\"text\", \"text\", \"text/plain\")",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "ab2311e752c1e3a2d46ff4128648d71213e7369c85e551d8fe90ba7ac8b39771": {
      "id": "config.yml:top_level_key:1:31fecb481a",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "config.yml",
        "start_line": 1,
        "end_line": 12
      },
      "text": "llm:\nendpoint: http://localhost:11434/v1/chat/completions\nmax_retries: 2\nmodel: qwen2.5-coder:7b-instruct\nprompt_version: chunk_enrich_v1",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "aed494c8e8ee8c69785ee2aecbdc832f0dda6641384362da3057c617e7de0702": {
      "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:42:77fe66d391",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/schemas.py",
        "start_line": 42,
        "end_line": 53
      },
      "text": "def to_dict(self) -> dict[str, Any]:\n        return {\n            \"id\": self.id,\n            \"kind\": self.kind,\n            \"text\": self.text,\n            \"start\": self.start_line,\n            \"end\": self.end_line,\n            \"symbol\": self.symbol,\n            \"metadata\": dict(self.metadata),\n        }\n\n    @classmethod",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "b5dab0ab13028c68665fd99da2b93154c1cd552ba417deb7d81a325d3c3f3f39": {
      "id": "cpm_llm_builder_plugin/features.py:code_symbol:193:7a3392cf78",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/features.py",
        "start_line": 193,
        "end_line": 204
      },
      "text": "def _fallback_chunk(self, *, source: SourceDocument, segment_text: str, segment_id: str, start: int, end: int) -> Chunk:\n        return Chunk(\n            id=segment_id,\n            text=segment_text,\n            title=\"\",\n            summary=\"\",\n            tags=(),\n            anchors={\"path\": source.path, \"start_line\": start, \"end_line\": end},\n            relations={},\n            metadata={\"fallback\": True},\n        )",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "b69f6b8b9bd2dbba1266713bb393393a400872431634b65745df48919c1298c4": {
      "id": "cpm_llm_builder_plugin/entrypoint.py:code_symbol:11:4a3baf7526",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/entrypoint.py",
        "start_line": 11,
        "end_line": 15
      },
      "text": "def init(self, ctx) -> None:\n        self.context = ctx\n        features.set_plugin_root(ctx.plugin_root)\n        _ = features.CPMLLMBuilder",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "b7360b3cc4a48e339a99de3cc1cbc245fcbc16dec9ff709a5e75b9f045474c5b": {
      "id": "cpm_llm_builder_plugin/prompts/chunk_enrich_v1.txt:paragraph:1:bf44f5a00a",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/prompts/chunk_enrich_v1.txt",
        "start_line": 1,
        "end_line": 10
      },
      "text": "You are a chunk enrichment model.\nGiven source metadata and deterministic segments, return JSON object with key \"chunks\".\nFor each segment return:\n- id\n- title\n- summary\n- tags (array of short tags)\n- anchors (path, start_line, end_line)\n- text (can be original segment text)\n- relations (calls, called_by, related_symbols if available)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "b99d7454eb483abde1299f6635fb5bc63fb839522effad8f9afb813cc4207c7e": {
      "id": "cpm_llm_builder_plugin/llm_client.py:code_symbol:74:e82b6b58e3",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/llm_client.py",
        "start_line": 74,
        "end_line": 74
      },
      "text": "class LLMClient:",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "b9b283b3f80dc222a8bc3e775de4afd50076f06b4d945b5b823e2a3f385b934f": {
      "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:135:fa6df14b44",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/schemas.py",
        "start_line": 135,
        "end_line": 174
      },
      "text": "def normalize_chunk_list(payload: Any) -> list[Chunk]:\n    \"\"\"Convert legacy/OpenAI-like payloads to chunks.\"\"\"\n\n    raw_chunks: Any = None\n\n    if isinstance(payload, list):\n        raw_chunks = payload\n    elif isinstance(payload, Mapping):\n        if \"chunks\" in payload:\n            raw_chunks = payload.get(\"chunks\")\n        elif \"output\" in payload:\n            output = payload.get(\"output\")\n            if isinstance(output, Sequence):\n                for item in output:\n                    if not isinstance(item, Mapping):\n                        continue\n                    if item.get(\"type\") != \"output_json\":\n                        continue\n                    json_payload = item.get(\"json\")\n                    if isinstance(json_payload, Mapping) and \"chunks\" in json_payload:\n                        raw_chunks = json_payload.get(\"chunks\")\n                        break\n\n    if not isinstance(raw_chunks, list):\n        raise ValueError(\"LLM response does not contain chunk list\")\n\n    chunks: list[Chunk] = []\n    for item in raw_chunks:\n        if isinstance(item, str):\n            text = item.strip()\n            if not text:\n                continue\n            chunks.append(Chunk(id=\"\", text=text))\n            continue\n        if isinstance(item, Mapping):\n            chunk = Chunk.from_dict(item)\n            if chunk.text.strip():\n                chunks.append(chunk)\n    return chunks",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "bdb3067adb185a79565c3b5ed898ce2d1e28657974413edc8f93afa1c998d4dd": {
      "id": "cpm_llm_builder_plugin/entrypoint.py:code_symbol:8:590d188334",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/entrypoint.py",
        "start_line": 8,
        "end_line": 10
      },
      "text": "class LLMBuilderEntrypoint:\n    \"\"\"Initialize plugin state and register features.\"\"\"",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "c52f45dc65ca36d99c9d4cd101d79be1307b3bf0ed471e6409e53a2b8b749f94": {
      "id": "cpm_llm_builder_plugin/llm_client.py:code_symbol:16:7ce4ede5ab",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/llm_client.py",
        "start_line": 16,
        "end_line": 24
      },
      "text": "class LLMClientConfig:\n    endpoint: str\n    model: str\n    request_timeout: float\n    prompt_version: str\n    max_retries: int = 2\n    retry_backoff_seconds: float = 0.5",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "c5320efd1e3ca48b786b3838131f8ac89dc5ea6d92a605990a0786f56f09d406": {
      "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:51:ba10266f03",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/prechunk.py",
        "start_line": 51,
        "end_line": 93
      },
      "text": "def _java_segments(path: str, content: str) -> list[Segment]:\n    lines = content.splitlines()\n    if not lines:\n        return []\n\n    ranges: list[tuple[str, int, int, str | None]] = []\n    current_start: int | None = None\n    current_kind = \"java_block\"\n    current_symbol: str | None = None\n    brace_depth = 0\n\n    for idx, line in enumerate(lines, start=1):\n        type_match = JAVA_TYPE_RE.match(line)\n        method_match = JAVA_METHOD_RE.match(line)\n        starts_symbol = type_match is not None or method_match is not None\n\n        if starts_symbol and current_start is None:\n            current_start = idx\n            if type_match is not None:\n                current_kind = \"class_header\"\n                current_symbol = type_match.group(3)\n            else:\n                current_kind = \"method\"\n                current_symbol = method_match.group(3) if method_match else None\n\n        brace_depth += line.count(\"{\")\n        brace_depth -= line.count(\"}\")\n\n        if current_start is not None and brace_depth <= 0:\n            ranges.append((current_kind, current_start, idx, current_symbol))\n            current_start = None\n            current_kind = \"java_block\"\n            current_symbol = None\n            brace_depth = 0\n\n    if current_start is not None:\n        ranges.append((current_kind, current_start, len(lines), current_symbol))\n\n    if not ranges:\n        ranges.append((\"java_file\", 1, len(lines), None))\n    return _split_by_ranges(lines, ranges, path)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "c648e5bc831904723be9969bd091f295afb3e7890b7e6e8fd4f63875ff5a21fa": {
      "id": "cpm_llm_builder_plugin/cache.py:code_symbol:29:ef5b2856da",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/cache.py",
        "start_line": 29,
        "end_line": 76
      },
      "text": "def load_cache(path: Path) -> CacheV2:\n    if not path.exists():\n        return CacheV2()\n    try:\n        payload = json.loads(path.read_text(encoding=\"utf-8\"))\n    except Exception:\n        return CacheV2()\n\n    if isinstance(payload, Mapping) and payload.get(\"version\") == CACHE_VERSION:\n        return _load_v2(payload)\n\n    # v1 migration: {\"files\": {\"path\": {\"source_hash\": \"...\", \"chunks\":[...]}}}\n    migrated = CacheV2()\n    files_raw = payload.get(\"files\") if isinstance(payload, Mapping) else None\n    if isinstance(files_raw, Mapping):\n        for rel, entry in files_raw.items():\n            if not isinstance(rel, str) or not isinstance(entry, Mapping):\n                continue\n            source_hash = entry.get(\"source_hash\")\n            chunks = entry.get(\"chunks\")\n            if not isinstance(source_hash, str):\n                continue\n            segments: list[Segment] = []\n            if isinstance(chunks, list):\n                for idx, item in enumerate(chunks):\n                    if not isinstance(item, str):\n                        continue\n                    text = item.strip()\n                    if not text:\n                        continue\n                    segments.append(\n                        Segment(\n                            id=f\"{rel}:legacy:{idx}\",\n                            kind=\"legacy_chunk\",\n                            text=text,\n                            start_line=1,\n                            end_line=1,\n                            metadata={},\n                        )\n                    )\n            migrated.files[rel] = FileCacheEntry(\n                source_hash=source_hash,\n                classification={\"pipeline\": \"legacy\"},\n                segments=segments,\n            )\n    return migrated",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "c6a8d54992fe4a327471ca3a9b8da1dfd950e0c871228adecbb93c0d5b14654c": {
      "id": "cpm_llm_builder_plugin/cache.py:code_symbol:24:ed746e53b3",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/cache.py",
        "start_line": 24,
        "end_line": 28
      },
      "text": "class CacheV2:\n    files: dict[str, FileCacheEntry] = field(default_factory=dict)\n    segment_enrichment: dict[str, Chunk] = field(default_factory=dict)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "cf6cd9eecc2ea9e657681fc51735fc44fe864bd9983cfbe029719b05d7955fe6": {
      "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:90:6d29319295",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/schemas.py",
        "start_line": 90,
        "end_line": 105
      },
      "text": "def from_dict(cls, payload: Mapping[str, Any]) -> \"Chunk\":\n        tags_raw = payload.get(\"tags\") or []\n        tags = tuple(str(item) for item in tags_raw if isinstance(item, str))\n        return cls(\n            id=str(payload.get(\"id\") or \"\"),\n            text=str(payload.get(\"text\") or \"\"),\n            title=str(payload.get(\"title\") or \"\"),\n            summary=str(payload.get(\"summary\") or \"\"),\n            tags=tags,\n            anchors=dict(payload.get(\"anchors\") or {}),\n            relations=dict(payload.get(\"relations\") or {}),\n            metadata=dict(payload.get(\"metadata\") or {}),\n        )\n\n\n@dataclass(frozen=True)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "d4ebb17c0bcd792432f214af762a882584cbbf7e0053418eccee51d5847d2941": {
      "id": "cpm_llm_builder_plugin/validators.py:code_symbol:12:0e442e8845",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/validators.py",
        "start_line": 12,
        "end_line": 16
      },
      "text": "class ValidationResult:\n    chunks: tuple[Chunk, ...]\n    warnings: tuple[str, ...]",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "d916b617a1cedb19ded8db16bc4082f901f911e4fea673bf3a3894423d850cda": {
      "id": "__init__.py:code_block:1:76eea5ae9d",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "__init__.py",
        "start_line": 1,
        "end_line": 2
      },
      "text": "\"\"\"LLM builder plugin package.\"\"\"",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "dd08183e349d4048913ef6cdb44aad8106c18d38b99104008a522eea8faee32f": {
      "id": "cpm_llm_builder_plugin/postprocess.py:code_symbol:10:0e7021dd26",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/postprocess.py",
        "start_line": 10,
        "end_line": 54
      },
      "text": "def _split_chunk(chunk: Chunk, max_tokens: int) -> list[Chunk]:\n    lines = chunk.text.splitlines()\n    if not lines:\n        return [chunk]\n    result: list[Chunk] = []\n    buffer: list[str] = []\n    part = 0\n    for line in lines:\n        candidate = \"\\n\".join(buffer + [line]).strip()\n        if buffer and estimate_tokens(candidate) > max_tokens:\n            text = \"\\n\".join(buffer).strip()\n            if text:\n                result.append(\n                    Chunk(\n                        id=f\"{chunk.id}:part:{part}\",\n                        text=text,\n                        title=chunk.title,\n                        summary=chunk.summary,\n                        tags=chunk.tags,\n                        anchors=dict(chunk.anchors),\n                        relations=dict(chunk.relations),\n                        metadata=dict(chunk.metadata),\n                    )\n                )\n                part += 1\n            buffer = [line]\n        else:\n            buffer.append(line)\n    final_text = \"\\n\".join(buffer).strip()\n    if final_text:\n        result.append(\n            Chunk(\n                id=f\"{chunk.id}:part:{part}\" if part else chunk.id,\n                text=final_text,\n                title=chunk.title,\n                summary=chunk.summary,\n                tags=chunk.tags,\n                anchors=dict(chunk.anchors),\n                relations=dict(chunk.relations),\n                metadata=dict(chunk.metadata),\n            )\n        )\n    return result or [chunk]",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "e41d4aaae361810a83c91bea19f9a1711f1a97ae26e4a57720993f8aea7606f6": {
      "id": "cpm_llm_builder_plugin/features.py:code_symbol:121:bc58ffc84a",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/features.py",
        "start_line": 121,
        "end_line": 143
      },
      "text": "def configure(cls, parser: argparse.ArgumentParser) -> None:\n        parser.add_argument(\"source\", help=\"Source directory to build\")\n        parser.add_argument(\"--destination\", required=True, help=\"Destination packet directory\")\n        parser.add_argument(\"--packet-version\", default=\"0.0.0\", help=\"Packet version for manifest/cpm.yml\")\n        parser.add_argument(\"--config\", help=\"Plugin config.yml path (default: plugin config.yml)\")\n        parser.add_argument(\"--llm-endpoint\", help=\"Override LLM endpoint from config\")\n        parser.add_argument(\"--request-timeout\", type=float, help=\"Timeout in seconds for LLM calls\")\n        parser.add_argument(\"--llm-model\", help=\"Override LLM model\")\n        parser.add_argument(\"--prompt-version\", help=\"Override prompt version\")\n        parser.add_argument(\"--max-retries\", type=int, help=\"LLM retries\")\n        parser.add_argument(\"--max-chunk-tokens\", type=int, help=\"Chunk hard max size\")\n        parser.add_argument(\"--min-chunk-tokens\", type=int, help=\"Chunk soft min size\")\n        parser.add_argument(\"--max-segments-per-request\", type=int, help=\"LLM batch size\")\n        parser.add_argument(\"--model-name\", default=DEFAULT_MODEL, help=\"Embedding model name\")\n        parser.add_argument(\"--max-seq-length\", type=int, default=1024, help=\"Embedding max sequence length\")\n        parser.add_argument(\"--embed-url\", default=DEFAULT_EMBED_URL, help=\"Embedding endpoint URL\")\n        parser.add_argument(\"--embeddings-mode\", choices=[\"http\", \"legacy\"], default=\"http\")\n        parser.add_argument(\"--timeout\", type=float, default=None, help=\"Embedding request timeout in seconds\")\n        parser.add_argument(\"--archive\", dest=\"archive\", action=\"store_true\", help=\"Create archive output\")\n        parser.add_argument(\"--no-archive\", dest=\"archive\", action=\"store_false\", help=\"Skip archive output\")\n        parser.set_defaults(archive=True)\n        parser.add_argument(\"--archive-format\", choices=[\"tar.gz\", \"zip\"], default=\"tar.gz\")",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "e6eba5133cdc750c26209747f63bce831539019223432ea318855ed4746b9737": {
      "id": "cpm_llm_builder_plugin/features.py:code_symbol:148:ffeafa9b5c",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/features.py",
        "start_line": 148,
        "end_line": 192
      },
      "text": "def run(self, argv: Sequence[str]) -> int:\n        args = argv\n        config_path = _resolve_config_path(getattr(args, \"config\", None))\n        if not config_path.exists():\n            print(f\"[error] config file not found: {config_path}\")\n            return 1\n        try:\n            base = LLMBuilderPluginConfig.from_path(config_path)\n        except Exception as exc:\n            print(f\"[error] invalid config.yml: {exc}\")\n            return 1\n\n        constraints = ChunkConstraints(\n            max_chunk_tokens=int(getattr(args, \"max_chunk_tokens\", None) or base.max_chunk_tokens),\n            min_chunk_tokens=int(getattr(args, \"min_chunk_tokens\", None) or base.min_chunk_tokens),\n            max_segments_per_request=int(\n                getattr(args, \"max_segments_per_request\", None) or base.max_segments_per_request\n            ),\n        )\n\n        runtime = LLMBuilderRuntimeConfig(\n            llm_endpoint=str(getattr(args, \"llm_endpoint\", None) or base.llm_endpoint),\n            request_timeout=float(getattr(args, \"request_timeout\", None) or base.request_timeout),\n            llm_model=str(getattr(args, \"llm_model\", None) or base.llm_model),\n            prompt_version=str(getattr(args, \"prompt_version\", None) or base.prompt_version),\n            max_retries=int(getattr(args, \"max_retries\", None) or base.max_retries),\n            constraints=constraints,\n            model_name=str(getattr(args, \"model_name\", None) or DEFAULT_MODEL),\n            max_seq_length=int(getattr(args, \"max_seq_length\", None) or 1024),\n            version=str(getattr(args, \"packet_version\", None) or \"0.0.0\"),\n            archive=bool(getattr(args, \"archive\", True)),\n            archive_format=str(getattr(args, \"archive_format\", None) or \"tar.gz\"),\n            embed_url=str(getattr(args, \"embed_url\", None) or DEFAULT_EMBED_URL),\n            embeddings_mode=str(getattr(args, \"embeddings_mode\", None) or \"http\"),\n            timeout=getattr(args, \"timeout\", None),\n        )\n        self.config = runtime\n        self.embedder = self.embedder or EmbeddingClient(\n            base_url=runtime.embed_url,\n            mode=runtime.embeddings_mode,\n            timeout_s=runtime.timeout,\n        )\n        manifest = self.build(str(getattr(args, \"source\")), destination=str(getattr(args, \"destination\")))\n        return 0 if manifest is not None else 1",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "e765b2c0aef6bfede52f73e66aaefb299fb07638edd9c96a8c018489c0f3a14f": {
      "id": "config.yml:top_level_key:1:57d4efb39a",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "config.yml",
        "start_line": 1,
        "end_line": 12
      },
      "text": "constraints:\nmax_chunk_tokens: 800\nmax_segments_per_request: 8\nmin_chunk_tokens: 120",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "e76b377b9389ad6ececc5b05cecd85e613919d703dd05c41257287e9fdf1355b": {
      "id": "cpm_llm_builder_plugin/features.py:code_symbol:51:5706db2674",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/features.py",
        "start_line": 51,
        "end_line": 54
      },
      "text": "def _sha256_text(value: str) -> str:\n    return hashlib.sha256(value.encode(\"utf-8\")).hexdigest()",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "e77e6c4fd3b684bbd48486171eea6ae5630e26d2fb9e5af5d363bcd29d4c1d53": {
      "id": "DESIGN.md:heading_section:3:7a1e8e779e",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "DESIGN.md",
        "start_line": 3,
        "end_line": 15
      },
      "text": "## Current Pipeline (v2 foundation)\n\n1. Ingest file text.\n2. Classify file type/language/mime.\n3. Deterministic pre-chunking (code/doc/json/text).\n4. LLM enrichment via OpenAI-like envelope over HTTP.\n5. Postprocess split/merge by token constraints.\n6. Validate chunks and collect warnings.\n7. Cache v2:\n   - file-level segmentation cache\n   - segment-level enrichment cache\n8. Produce CPM packet artifacts with embedding incremental reuse.",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "e7b1099f7603b5ebc283037cc7c8b56ef21c2b40e6bf247a3ebc0fc2b2719235": {
      "id": "cpm_llm_builder_plugin/llm_client.py:code_symbol:25:03f65379d2",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/llm_client.py",
        "start_line": 25,
        "end_line": 34
      },
      "text": "def _prompt_text(prompt_version: str) -> str:\n    return (\n        \"You are a chunk enrichment model. \"\n        f\"Use prompt version {prompt_version}. \"\n        \"Return strict JSON with key 'chunks'. \"\n        \"For each input segment produce chunk fields: \"\n        \"id,title,summary,tags,anchors,text,relations.\"\n    )",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "e7c59ec5e188fbfdff56d6e3cc1aa5d3b52d35457040537fd6df5bada06db3b5": {
      "id": "cpm_llm_builder_plugin/features.py:code_symbol:64:1efecd58ec",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/features.py",
        "start_line": 64,
        "end_line": 74
      },
      "text": "class LLMBuilderPluginConfig:\n    llm_endpoint: str\n    request_timeout: float = 30.0\n    llm_model: str = \"chunker-xxx\"\n    prompt_version: str = \"chunk_enrich_v1\"\n    max_retries: int = 2\n    max_chunk_tokens: int = 800\n    min_chunk_tokens: int = 120\n    max_segments_per_request: int = 8\n\n    @classmethod",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "e8905df330810c7cd5a599eb640f81da903ae6369f268f5a395db7f3c211e436": {
      "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:131:4fefa83a28",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/prechunk.py",
        "start_line": 131,
        "end_line": 148
      },
      "text": "def _json_yaml_segments(path: str, content: str, *, is_yaml: bool) -> list[Segment]:\n    try:\n        parsed = yaml.safe_load(content) if is_yaml else json.loads(content)\n    except Exception:\n        return [_segment(path, \"structured_blob\", content, 1, max(len(content.splitlines()), 1), None)]\n    lines = content.splitlines()\n    if not isinstance(parsed, dict):\n        return [_segment(path, \"structured_blob\", content, 1, max(len(lines), 1), None)]\n    if not lines:\n        return []\n    segments: list[Segment] = []\n    for key, value in parsed.items():\n        value_text = json.dumps(value, ensure_ascii=False, indent=2) if not is_yaml else yaml.safe_dump(value)\n        text = f\"{key}:\\n{value_text}\".strip()\n        segments.append(_segment(path, \"top_level_key\", text, 1, len(lines), str(key)))\n    return segments",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "ea33c404e073bfb0a0fbd7fe634682b528ab7cb85433aa3edb14da1da24035f2": {
      "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:39:d54e87eb8d",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/prechunk.py",
        "start_line": 39,
        "end_line": 50
      },
      "text": "def _split_by_ranges(lines: list[str], ranges: Iterable[tuple[str, int, int, str | None]], path: str) -> list[Segment]:\n    segments: list[Segment] = []\n    for kind, start, end, symbol in ranges:\n        start_idx = max(start - 1, 0)\n        end_idx = min(end, len(lines))\n        text = \"\\n\".join(lines[start_idx:end_idx]).strip()\n        if not text:\n            continue\n        segments.append(_segment(path, kind, text, start, end, symbol))\n    return segments",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "eabc23c76942a88c291ddf8a740dca010ede04e0467be0e0f67904bda0daae3d": {
      "id": "cpm_llm_builder_plugin/cache.py:code_symbol:77:d915515a66",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/cache.py",
        "start_line": 77,
        "end_line": 114
      },
      "text": "def _load_v2(payload: Mapping[str, Any]) -> CacheV2:\n    result = CacheV2()\n    files_raw = payload.get(\"files\")\n    if isinstance(files_raw, Mapping):\n        for rel, value in files_raw.items():\n            if not isinstance(rel, str) or not isinstance(value, Mapping):\n                continue\n            source_hash = value.get(\"source_hash\")\n            if not isinstance(source_hash, str):\n                continue\n            cls = dict(value.get(\"classification\") or {})\n            seg_payload = value.get(\"segments\") or []\n            segments = []\n            if isinstance(seg_payload, list):\n                for item in seg_payload:\n                    if isinstance(item, Mapping):\n                        try:\n                            segments.append(Segment.from_dict(item))\n                        except Exception:\n                            continue\n            result.files[rel] = FileCacheEntry(\n                source_hash=source_hash,\n                classification=cls,\n                segments=segments,\n            )\n\n    seg_enrichment = payload.get(\"segment_enrichment\")\n    if isinstance(seg_enrichment, Mapping):\n        for key, value in seg_enrichment.items():\n            if not isinstance(key, str) or not isinstance(value, Mapping):\n                continue\n            try:\n                result.segment_enrichment[key] = Chunk.from_dict(value)\n            except Exception:\n                continue\n    return result",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "ead6b499d3d85084d4c5fcc469bd12797688aef0451434cfac359bb5764ee32d": {
      "id": "cpm_llm_builder_plugin/features.py:code_symbol:205:8aa6297f8f",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/features.py",
        "start_line": 205,
        "end_line": 527
      },
      "text": "def build(self, source: str, *, destination: str | None = None) -> PacketManifest | None:\n        if self.config is None:\n            raise ValueError(\"runtime config is not initialized\")\n        if self.embedder is None:\n            self.embedder = EmbeddingClient(\n                base_url=self.config.embed_url,\n                mode=self.config.embeddings_mode,\n                timeout_s=self.config.timeout,\n            )\n\n        source_path = Path(source).resolve()\n        if not source_path.exists():\n            print(f\"[error] source '{source_path}' does not exist\")\n            return None\n        if destination is None:\n            raise ValueError(\"destination path must be provided\")\n        out_root = Path(destination).resolve()\n        out_root.mkdir(parents=True, exist_ok=True)\n        (out_root / \"faiss\").mkdir(parents=True, exist_ok=True)\n        print(f\"[build] input_dir  = {source_path}\")\n        print(f\"[build] output_dir = {out_root}\")\n\n        llm_client = LLMClient(\n            LLMClientConfig(\n                endpoint=self.config.llm_endpoint,\n                model=self.config.llm_model,\n                request_timeout=self.config.request_timeout,\n                prompt_version=self.config.prompt_version,\n                max_retries=self.config.max_retries,\n            )\n        )\n\n        chunk_cache_path = out_root / CHUNK_CACHE_NAME\n        cache = load_cache(chunk_cache_path)\n        next_cache = CacheV2()\n        next_cache.segment_enrichment.update(cache.segment_enrichment)\n\n        chunks: list[DocChunk] = []\n        ext_counts: dict[str, int] = {}\n        files_indexed = 0\n        llm_calls = 0\n        file_cache_hits = 0\n        segment_cache_hits = 0\n\n        rel_root = source_path.resolve()\n        for file_path in sorted(source_path.rglob(\"*\")):\n            if not file_path.is_file():\n                continue\n            if file_path.suffix.lower() not in SUPPORTED_EXTS:\n                continue\n\n            text = _read_text_file(file_path)\n            if not text.strip():\n                continue\n            files_indexed += 1\n\n            rel = str(file_path.resolve().relative_to(rel_root)).replace(\"\\\\\", \"/\")\n            ext = file_path.suffix.lower()\n            ext_counts[ext] = ext_counts.get(ext, 0) + 1\n\n            classification = classify_file(file_path, text)\n            if not classification.is_supported_text:\n                continue\n\n            source_hash = _sha256_text(text)\n            cached_file = cache.files.get(rel)\n            if cached_file and cached_file.source_hash == source_hash:\n                segments = cached_file.segments\n                file_cache_hits += 1\n            else:\n                segments = prechunk(rel, text, classification)\n\n            next_cache.files[rel] = FileCacheEntry(\n                source_hash=source_hash,\n                classification={\n                    \"pipeline\": classification.pipeline,\n                    \"language\": classification.language,\n                    \"mime\": classification.mime,\n                },\n                segments=list(segments),\n            )\n            if not segments:\n                continue\n\n            source_doc = SourceDocument(\n                path=rel,\n                language=classification.language,\n                mime=classification.mime,\n                source_hash=source_hash,\n            )\n\n            resolved_chunks: list[Chunk] = []\n            missing_segments = []\n            missing_keys = []\n            for segment in segments:\n                key = segment_cache_key(\n                    segment=segment,\n                    model=self.config.llm_model,\n                    prompt_version=self.config.prompt_version,\n                    constraints=self.config.constraints,\n                )\n                cached = cache.segment_enrichment.get(key)\n                if cached is not None:\n                    resolved_chunks.append(cached)\n                    segment_cache_hits += 1\n                else:\n                    missing_segments.append(segment)\n                    missing_keys.append(key)\n\n            max_batch = max(1, self.config.constraints.max_segments_per_request)\n            for start in range(0, len(missing_segments), max_batch):\n                segment_batch = missing_segments[start : start + max_batch]\n                key_batch = missing_keys[start : start + max_batch]\n                try:\n                    enriched = llm_client.enrich(\n                        source=source_doc,\n                        segments=segment_batch,\n                        constraints=self.config.constraints,\n                    )\n                    llm_calls += 1\n                except Exception as exc:\n                    print(f\"[warn] llm enrichment failed for {rel}: {exc}; fallback enabled\")\n                    enriched = [\n                        self._fallback_chunk(\n                            source=source_doc,\n                            segment_text=segment.text,\n                            segment_id=segment.id,\n                            start=segment.start_line,\n                            end=segment.end_line,\n                        )\n                        for segment in segment_batch\n                    ]\n                for key, enriched_chunk in zip(key_batch, enriched):\n                    next_cache.segment_enrichment[key] = enriched_chunk\n                    resolved_chunks.append(enriched_chunk)\n\n            post = apply_chunk_constraints(resolved_chunks, self.config.constraints)\n            validation = validate_chunks(post)\n            for warning in validation.warnings:\n                print(f\"[warn] {rel}: {warning}\")\n\n            for chunk in validation.chunks:\n                meta = dict(chunk.metadata)\n                meta.update(\n                    {\n                        \"path\": rel,\n                        \"ext\": ext,\n                        \"title\": chunk.title,\n                        \"summary\": chunk.summary,\n                        \"tags\": list(chunk.tags),\n                        \"anchors\": dict(chunk.anchors),\n                        \"relations\": dict(chunk.relations),\n                    }\n                )\n                chunks.append(DocChunk(id=chunk.id, text=chunk.text, metadata=meta))\n\n        save_cache(chunk_cache_path, next_cache)\n        print(f\"[scan] files_indexed={files_indexed}\")\n        print(f\"[scan] chunks_total={len(chunks)}\")\n        print(f\"[chunk] llm_calls={llm_calls} file_cache_hits={file_cache_hits} segment_cache_hits={segment_cache_hits}\")\n        if not chunks:\n            print(\"[error] No chunks found.\")\n            return None\n\n        cache_pack = _load_existing_cache(\n            out_root,\n            model_name=self.config.model_name,\n            max_seq_length=self.config.max_seq_length,\n        )\n        cache_vecs: dict[str, np.ndarray] = {}\n        cache_dim: Optional[int] = None\n        if cache_pack:\n            cache_vecs, cache_dim = cache_pack\n            print(f\"[cache] enabled: cached_vectors={len(cache_vecs)} dim={cache_dim}\")\n        else:\n            print(\"[cache] disabled (no compatible previous build found)\")\n\n        new_hashes = [_chunk_hash(chunk.text) for chunk in chunks]\n        prev_set = set(cache_vecs.keys())\n        new_set = set(new_hashes)\n        removed = len(prev_set - new_set) if cache_vecs else 0\n        reused = sum(1 for hsh in new_hashes if hsh in cache_vecs)\n\n        to_embed_idx: list[int] = []\n        to_embed_texts: list[str] = []\n        for idx, hsh in enumerate(new_hashes):\n            if hsh not in cache_vecs:\n                to_embed_idx.append(idx)\n                to_embed_texts.append(chunks[idx].text)\n        print(f\"[cache] new_chunks={len(chunks)} reused={reused} to_embed={len(to_embed_idx)} removed={removed}\")\n\n        if not self.embedder.health():\n            print(\n                f\"[error] embedding server not reachable at {self.config.embed_url} (mode={self.config.embeddings_mode})\"\n            )\n            return None\n\n        vec_missing: Optional[np.ndarray] = None\n        dim: Optional[int] = cache_dim\n        if to_embed_texts:\n            vec_missing = self.embedder.embed_texts(\n                to_embed_texts,\n                model_name=self.config.model_name,\n                max_seq_length=self.config.max_seq_length,\n                normalize=True,\n                dtype=\"float32\",\n                show_progress=True,\n            )\n            dim = int(vec_missing.shape[1])\n        elif dim is None and chunks:\n            vec_missing = self.embedder.embed_texts(\n                [chunks[0].text],\n                model_name=self.config.model_name,\n                max_seq_length=self.config.max_seq_length,\n                normalize=True,\n                dtype=\"float32\",\n                show_progress=False,\n            )\n            dim = int(vec_missing.shape[1])\n            to_embed_idx = [0]\n        assert dim is not None\n\n        if cache_dim is not None and cache_dim != dim:\n            cache_vecs = {}\n            reused = 0\n            to_embed_idx = list(range(len(chunks)))\n            to_embed_texts = [chunk.text for chunk in chunks]\n            vec_missing = self.embedder.embed_texts(\n                to_embed_texts,\n                model_name=self.config.model_name,\n                max_seq_length=self.config.max_seq_length,\n                normalize=True,\n                dtype=\"float32\",\n                show_progress=True,\n            )\n            dim = int(vec_missing.shape[1])\n\n        final_vecs = np.empty((len(chunks), dim), dtype=np.float32)\n        if cache_vecs:\n            for idx, hsh in enumerate(new_hashes):\n                vector = cache_vecs.get(hsh)\n                if vector is not None:\n                    final_vecs[idx] = vector\n        if to_embed_idx:\n            assert vec_missing is not None\n            for miss_idx, chunk_idx in enumerate(to_embed_idx):\n                final_vecs[chunk_idx] = vec_missing[miss_idx]\n\n        docs_path = out_root / \"docs.jsonl\"\n        write_docs_jsonl(chunks, docs_path)\n        db = FaissFlatIP(dim=dim)\n        db.add(final_vecs)\n        db_path = out_root / \"faiss\" / \"index.faiss\"\n        db.save(str(db_path))\n        vectors_path = out_root / \"vectors.f16.bin\"\n        write_vectors_f16(final_vecs, vectors_path)\n\n        tags = _infer_tags(ext_counts)\n        _write_cpm_yml(\n            out_root,\n            name=out_root.name,\n            version=self.config.version,\n            description=source_path.as_posix(),\n            tags=tags,\n            entrypoints=[\"query\"],\n            embedding_model=self.config.model_name,\n            embedding_dim=dim,\n            embedding_normalized=True,\n        )\n\n        manifest = PacketManifest(\n            schema_version=\"1.0\",\n            packet_id=out_root.name,\n            embedding=EmbeddingSpec(\n                provider=\"sentence-transformers\",\n                model=self.config.model_name,\n                dim=dim,\n                dtype=\"float16\",\n                normalized=True,\n                max_seq_length=self.config.max_seq_length,\n            ),\n            similarity={\n                \"space\": \"cosine\",\n                \"index_type\": \"faiss.IndexFlatIP\",\n                \"notes\": \"cosine via inner product on normalized vectors\",\n            },\n            files={\n                \"docs\": \"docs.jsonl\",\n                \"vectors\": {\"path\": \"vectors.f16.bin\", \"format\": \"f16_rowmajor\"},\n                \"index\": {\"path\": \"faiss/index.faiss\", \"format\": \"faiss\"},\n                \"chunk_cache\": CHUNK_CACHE_NAME,\n                \"calibration\": None,\n            },\n            counts={\"docs\": len(chunks), \"vectors\": int(db.index.ntotal)},\n            source={\"input_dir\": source_path.as_posix(), \"file_ext_counts\": ext_counts},\n            cpm={\n                \"name\": out_root.name,\n                \"version\": self.config.version,\n                \"tags\": tags,\n                \"entrypoints\": [\"query\"],\n            },\n            incremental={\n                \"enabled\": bool(cache_pack or cache.files),\n                \"reused\": reused,\n                \"embedded\": len(to_embed_idx),\n                \"removed\": removed,\n                \"file_cache_hits\": file_cache_hits,\n                \"segment_cache_hits\": segment_cache_hits,\n                \"llm_calls\": llm_calls,\n            },\n        )\n        manifest.checksums = compute_checksums(\n            out_root,\n            [\"cpm.yml\", \"docs.jsonl\", \"vectors.f16.bin\", \"faiss/index.faiss\", CHUNK_CACHE_NAME],\n        )\n        manifest_path = out_root / \"manifest.json\"\n        write_manifest(manifest, manifest_path)\n\n        if self.config.archive:\n            archive_path = _archive_packet_dir(out_root, self.config.archive_format)\n            print(f\"[write] archive -> {archive_path}\")\n        print(\"[done] build ok\")\n        return manifest",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "eba744db837f6da27d24825a656e3b8ea904491328f71fdabc2a26e832061cc6": {
      "id": "README.md:heading_section:20:0ea2c21c7d",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "README.md",
        "start_line": 20,
        "end_line": 61
      },
      "text": "## Endpoint Contract\n\nThe plugin sends an OpenAI-like envelope:\n\n```json\n{\n  \"model\": \"chunker-xxx\",\n  \"input\": [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": \"instructions\"},\n        {\n          \"type\": \"input_json\",\n          \"json\": {\n            \"task\": \"chunk.enrich\",\n            \"source\": {\"path\": \"...\", \"language\": \"java\", \"mime\": \"text/x-java\", \"hash\": \"...\"},\n            \"segments\": [{\"id\": \"...\", \"kind\": \"method\", \"text\": \"...\", \"start\": 10, \"end\": 20}],\n            \"constraints\": {\"max_chunk_tokens\": 800, \"min_chunk_tokens\": 120, \"max_segments_per_request\": 8}\n          }\n        }\n      ]\n    }\n  ],\n  \"metadata\": {\"cpm_plugin\": \"cpm-llm-builder\", \"prompt_version\": \"chunk_enrich_v1\"}\n}\n```\n\nAccepted response formats:\n- OpenAI-like:\n```json\n{\"output\":[{\"type\":\"output_json\",\"json\":{\"chunks\":[{\"id\":\"...\",\"text\":\"...\"}]}}]}\n```\n- Legacy:\n```json\n{\"chunks\":[{\"id\":\"...\",\"text\":\"...\"}]}\n```\nor\n```json\n[\"chunk text 1\", \"chunk text 2\"]\n```",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "ec08f5f16ad8885a5f00eae956c0428f4266c41936d862b5674b9512ea559504": {
      "id": "README.md:heading_section:1:58a8c5e3a1",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "README.md",
        "start_line": 1,
        "end_line": 4
      },
      "text": "# cpm-llm-builder\n\n`cpm-llm-builder` is a CPM plugin that builds packets with a deterministic pre-chunk pipeline and LLM enrichment over HTTP.",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "ed7fca46e0c9e6ab8ce3ce1faa29e7652747cf312c85941e0f9ac82d86346e7c": {
      "id": "cpm_llm_builder_plugin/classifiers.py:code_symbol:10:25cda329cb",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/classifiers.py",
        "start_line": 10,
        "end_line": 41
      },
      "text": "class FileClassification:\n    pipeline: str\n    language: str\n    mime: str\n    is_supported_text: bool = True\n\n\nPIPELINES_BY_EXT: dict[str, FileClassification] = {\n    \".java\": FileClassification(\"java\", \"java\", \"text/x-java\"),\n    \".md\": FileClassification(\"markdown\", \"markdown\", \"text/markdown\"),\n    \".markdown\": FileClassification(\"markdown\", \"markdown\", \"text/markdown\"),\n    \".html\": FileClassification(\"html\", \"html\", \"text/html\"),\n    \".htm\": FileClassification(\"html\", \"html\", \"text/html\"),\n    \".json\": FileClassification(\"json\", \"json\", \"application/json\"),\n    \".yml\": FileClassification(\"yaml\", \"yaml\", \"application/yaml\"),\n    \".yaml\": FileClassification(\"yaml\", \"yaml\", \"application/yaml\"),\n    \".txt\": FileClassification(\"text\", \"text\", \"text/plain\"),\n    \".rst\": FileClassification(\"text\", \"rst\", \"text/plain\"),\n    \".py\": FileClassification(\"code_generic\", \"python\", \"text/x-python\"),\n    \".js\": FileClassification(\"code_generic\", \"javascript\", \"text/javascript\"),\n    \".ts\": FileClassification(\"code_generic\", \"typescript\", \"text/typescript\"),\n    \".tsx\": FileClassification(\"code_generic\", \"typescript\", \"text/typescript\"),\n    \".go\": FileClassification(\"code_generic\", \"go\", \"text/x-go\"),\n    \".rs\": FileClassification(\"code_generic\", \"rust\", \"text/x-rust\"),\n    \".c\": FileClassification(\"code_generic\", \"c\", \"text/x-c\"),\n    \".cpp\": FileClassification(\"code_generic\", \"cpp\", \"text/x-c++\"),\n    \".h\": FileClassification(\"code_generic\", \"c\", \"text/x-c\"),\n    \".cs\": FileClassification(\"code_generic\", \"csharp\", \"text/x-csharp\"),\n    \".kt\": FileClassification(\"code_generic\", \"kotlin\", \"text/x-kotlin\"),\n}",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "ee0eb0d848f48ed99d7364c03ee1b0b05cd58c6a30c6316737806c0f6284ea52": {
      "id": "README.md:heading_section:79:a19114bd3a",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "README.md",
        "start_line": 79,
        "end_line": 83
      },
      "text": "## Run\n\n```bash\ncpm llm:cpm-llm-builder ./docs --destination ./out/packet\n```",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "ee8ba4e2657512d1105701ac52b9c1448269805b9044c3caa63cb689f269b1c2": {
      "id": "cpm_llm_builder_plugin/llm_client.py:code_symbol:131:ea37ec79aa",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/llm_client.py",
        "start_line": 131,
        "end_line": 159
      },
      "text": "def _ensure_chunk_defaults(chunks: Sequence[Chunk], *, segments: Sequence[Segment], source: SourceDocument) -> list[Chunk]:\n    index: dict[str, Segment] = {segment.id: segment for segment in segments}\n    resolved: list[Chunk] = []\n    for pos, chunk in enumerate(chunks):\n        segment = index.get(chunk.id) if chunk.id else None\n        if segment is None and pos < len(segments):\n            segment = segments[pos]\n        chunk_id = chunk.id or (segment.id if segment else f\"{source.path}:chunk:{pos}\")\n        anchors = dict(chunk.anchors)\n        if \"path\" not in anchors:\n            anchors[\"path\"] = source.path\n        if segment is not None:\n            anchors.setdefault(\"start_line\", segment.start_line)\n            anchors.setdefault(\"end_line\", segment.end_line)\n        text = chunk.text.strip() or (segment.text if segment else \"\")\n        resolved.append(\n            Chunk(\n                id=chunk_id,\n                text=text,\n                title=chunk.title,\n                summary=chunk.summary,\n                tags=chunk.tags,\n                anchors=anchors,\n                relations=dict(chunk.relations),\n                metadata=dict(chunk.metadata),\n            )\n        )\n    return resolved",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "eebabe2658a5477e6a17eb3bec996a8b3a5fa34f6141c669609feb8e1b2bc3fb": {
      "id": "README.md:heading_section:5:a207346546",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "README.md",
        "start_line": 5,
        "end_line": 19
      },
      "text": "## Pipeline\n\n1. Ingest files.\n2. Classify file type/language/mime.\n3. Deterministic pre-chunk:\n   - `java`\n   - `code_generic`\n   - `markdown/html`\n   - `json/yaml`\n   - `text`\n4. LLM enrichment (metadata, title, summary, tags, relations).\n5. Postprocess split/merge by token constraints.\n6. Quality validation.\n7. Cache v2 (file + segment enrichment).",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "efa4024e2e92cbd7f2a27b126e30eaeaca0bcaef063926a822caffe606e1ca6f": {
      "id": "cpm_llm_builder_plugin/prechunk.py:code_symbol:27:dc8b0fb927",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/prechunk.py",
        "start_line": 27,
        "end_line": 38
      },
      "text": "def _segment(path: str, kind: str, text: str, start: int, end: int, symbol: str | None = None) -> Segment:\n    return Segment(\n        id=_make_id(path, kind, start, text),\n        kind=kind,\n        text=text.strip(),\n        start_line=start,\n        end_line=end,\n        symbol=symbol,\n        metadata={},\n    )",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "efce0405c5b9c960a3751ed8bf2185b0e3a371d93a0bb4676cdf164c3db4b6f3": {
      "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:106:d00d89d3db",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/schemas.py",
        "start_line": 106,
        "end_line": 110
      },
      "text": "class ChunkConstraints:\n    max_chunk_tokens: int = 800\n    min_chunk_tokens: int = 120\n    max_segments_per_request: int = 8",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "f5d72ff98c3fd8c8a0f436923c0edae4498fbc4c9bc6193c8e833e74c6099513": {
      "id": "cpm_llm_builder_plugin/__init__.py:code_block:1:7d2fad9b79",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/__init__.py",
        "start_line": 1,
        "end_line": 2
      },
      "text": "\"\"\"CPM plugin that chunks files with an external LLM endpoint before embedding.\"\"\"",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "f645d512b64da28a9aef1b7a6d6aa2e44f1e33d1d09a4c605e1138d5c47a9f2c": {
      "id": "DESIGN.md:heading_section:23:8430825f5e",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "DESIGN.md",
        "start_line": 23,
        "end_line": 26
      },
      "text": "## Future\n\n- Java AST parser integration (tree-sitter) can replace regex pre-chunker.\n- Circuit breaker and advanced transport options can be layered into `llm_client.py`.",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "fce5ed7a91e97418d30d8d023d75560679c3cb01aec792dfdcdec48f528cd1ef": {
      "id": "cpm_llm_builder_plugin/schemas.py:code_symbol:54:e09f116b8d",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/schemas.py",
        "start_line": 54,
        "end_line": 66
      },
      "text": "def from_dict(cls, payload: Mapping[str, Any]) -> \"Segment\":\n        return cls(\n            id=str(payload[\"id\"]),\n            kind=str(payload.get(\"kind\") or \"segment\"),\n            text=str(payload.get(\"text\") or \"\"),\n            start_line=int(payload.get(\"start\", 1)),\n            end_line=int(payload.get(\"end\", payload.get(\"start\", 1))),\n            symbol=str(payload[\"symbol\"]) if payload.get(\"symbol\") else None,\n            metadata=dict(payload.get(\"metadata\") or {}),\n        )\n\n\n@dataclass(frozen=True)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    },
    "fd427a72c5a80714fc5bc3e56bf12ca1b7a7b59b5e9c1bf8b83393a88a8294c2": {
      "id": "cpm_llm_builder_plugin/features.py:code_symbol:75:1863a044ea",
      "title": "",
      "summary": "",
      "tags": [],
      "anchors": {
        "path": "cpm_llm_builder_plugin/features.py",
        "start_line": 75,
        "end_line": 100
      },
      "text": "def from_path(cls, path: Path) -> \"LLMBuilderPluginConfig\":\n        payload = yaml.safe_load(path.read_text(encoding=\"utf-8\")) or {}\n        if not isinstance(payload, dict):\n            raise ValueError(\"config.yml must contain a mapping\")\n\n        legacy_endpoint = str(payload.get(\"llm_endpoint\") or \"\").strip()\n        llm_cfg = payload.get(\"llm\") if isinstance(payload.get(\"llm\"), dict) else {}\n        endpoint = str(llm_cfg.get(\"endpoint\") or legacy_endpoint).strip()\n        if not endpoint:\n            raise ValueError(\"config.yml must define llm.endpoint or llm_endpoint\")\n\n        constraints = payload.get(\"constraints\") if isinstance(payload.get(\"constraints\"), dict) else {}\n\n        return cls(\n            llm_endpoint=endpoint,\n            request_timeout=float(payload.get(\"request_timeout\", 30.0)),\n            llm_model=str(llm_cfg.get(\"model\") or \"chunker-xxx\"),\n            prompt_version=str(llm_cfg.get(\"prompt_version\") or \"chunk_enrich_v1\"),\n            max_retries=int(llm_cfg.get(\"max_retries\", 2)),\n            max_chunk_tokens=int(constraints.get(\"max_chunk_tokens\", 800)),\n            min_chunk_tokens=int(constraints.get(\"min_chunk_tokens\", 120)),\n            max_segments_per_request=int(constraints.get(\"max_segments_per_request\", 8)),\n        )\n\n\n@dataclass(frozen=True)",
      "relations": {},
      "metadata": {
        "fallback": true
      }
    }
  }
}